\chapter{Classification of Comparative Sentences}
The data collected from the crowdsourcing task was used as training data for two classification problems. In the first problem, a machine learning algorithm was trained to predict one of the three classes per sentence (see table \ref{tbl:mainstudy-classes}). The second problem is a simplification of the first one as it is designed as a binary classification problem. The classes \texttt{BETTER} and \texttt{WORSE} were merged into the class \texttt{ARG}.

The data was split into a training set (5759 sentences; 4194 \texttt{NONE}, 1091 \texttt{BETTER} and 474 \texttt{WORSE}) and a held-out set.
The experiments were conducted on the training set only. During the development, the experiments were evaluated using stratified k-fold cross-validation where k equals five. 

The held-out set stayed untouched until the final evaluation presented in section \ref{sec:final}.

If not stated otherwise, scikit-learn (\cite{scikit-learn} was used to perform feature processing, the classification and evaluation.

\section{Choice of Algorithms}

\begin{figure}[htb]
\centering
\caption{F1 score of all tested classification algorithms. A binary bag-of-words feature was used as the baseline feature. Each algorithm was trained with five stratified folds of the data. The black bars show the standard derivation.}
\label{tbl:algo}
\includegraphics[width=0.8\linewidth]{images/classifier}
\end{figure}

To find the most suitable classification algorithms, thirteen (see figure \ref{tbl:algo}) were selected and compared. Except \emph{XGBoost}\footnote{XGBoost is not part of scikit-learn. The implementation presented in \cite{DBLP:journals/corr/ChenG16} was used.} and \emph{Extra Trees Classifier}, all algorithms were used in at least one paper presented in section \ref{sec:argmine}. A binary bag-of-words model computed on the whole sentence (see section \ref{sec:features}) was used as the sole feature. The f1 score was used as the metric to compare the algorithms. 

Tree-based methods and linear models worked good. Support Vector Machines with non-linear kernels assigned \texttt{NONE} to all sentences.

As XGBoost and Logistic Regression already work in a pleasing way, no further investigations on the performance of other algorithms was done. A set of hyper-parameters for XGBoost was tested using exhaustive grid search and randomized search. However, no significant increase in the f1 score could be achieved.

In the following sections, all experiments were conducted using XGBoost with 1000 estimators.


\section{Features}
\label{sec:features}
Several vector representations were tested as features. The simplest one was a binary bag-of-words model realised with scikit-learns \texttt{CountVectorizer}. Another vector representation was generated with the five-hundred most frequent part-of-speech bi-, tri and four-grams. The mean word embedding vector was created by calculating the mean of each word's GloVe vector (as contained in spaCy's \texttt{en\_core\_web\_lg}\footnote{\url{https://spacy.io/models/en\#section-en\_core\_web\_lg}} model). Also, the pretrained InferSent model was used to create sentence embedding vectors.

A boolean feature capturing the appearance of a comparative adjective\footnote{Tag \emph{JJR} in the Penn Treebank (\url{https://www.ling.upenn.edu/courses/Fall\_2003/ling001/penn\_treebank\_pos.html})} was tested as well. Part-of-speech tagging for all features was done using spaCy.

Two preprocessing steps were used to generate the input for the feature calculation.

\begin{table}[ht]
\centering

\caption{Preprocessing examples for the sentence \enquote{\emph{In my mind, Python is better than Ruby}}}
\label{preprocessing_example}
\begin{tabularx}{\linewidth}{llX}
\toprule
Step 1 & Step 2 & Result \\ \midrule
Middle part & untouched & Python is better than Ruby \\
Middle part & removal & is better than \\
Full sentence & distinct replacement &In my mind, OBJECT\_A is better than OBJECT\_B \\
First part & removal & In my mind, \\
\bottomrule
\end{tabularx}

\end{table}

The first preprocessing step decided if the full sentence or a part of it should be used. The \emph{first part} contains all words from the beginning of the sentence to the first object, while the \emph{last part} contains all words from the second object to the end of the sentence. The \emph{middle part} contains all words between the first and the second object.

The second step was done to assess how important the objects are for the classification. The objects either stayed untouched, were removed or replaced. Two different replacement strategies were tested. First, both objects were replaced by the term \emph{OBJECT} (\emph{replacement}). Second, the first object was replaced by \emph{OBJECT\_A} and the second by \emph{OBJECT\_B} (\emph{distinct replacement}). This results in sixteen versions of each of the features mentioned above (four parts $\times$ four object strategies). Some examples are shown in table \ref{preprocessing_example}.\newline


\label{sec:lexnet_feat_desc}
Two features based on LexNet were created to encode dependency parsing information. The original code of LexNet was used to create the string representation of paths, as described in section \ref{sec:lexnet}. An LSTM was used to create path embeddings out of the string paths. Because the paper does not mention any details about the LSTM encoder, different architectures and hyper-parameter values were tested. The best results were achieved with the architecture described in figure \ref{fig:lexnetnn}. The batch  size was set to 128, \emph{RMSprop} (cite) was used as the optimizer. The network was trained for 150 epochs.

The addition of more layers or neurons did not increase the performance of the network. This is also true for adding bidirectionality to the LSTM layer. The paths (encoded as one-hot vectors) were used as targets for the network. Keras' embedding layer was used to create word embeddings of length 100 out of the path components prior to the path embeddings.

\begin{figure}[htbp]
\centering
\caption{Architecture of the LSTM path encoder. The embedding layer creates word embeddings for the path parts, which are fed into an LSTM with 200 neurons. The result is pooled and fed into a softmax layer. Keras \cite{chollet2015keras} was used to implement the network. The network was trained for 150 epochs with a batch size of 128 and RMSprop as the optimizer.}
\label{fig:lexnetnn}
\includegraphics{images/lex_arch}
\end{figure}



Different setups for the string path creation were tested. In the original implementation, the paths are restricted to a length of four. The directionality of edges is restricted as well. The first object must be reachable from the lowest common head of the two objects by following left edges only, the second one by following right edges. In the following, this setup is refered as \emph{original}. However, only 1502 sentences from the training set get a path with this restrictions. 

To overcome this problem, the restrictions were relaxed. The second LexNet setup (called \emph{customised}) limits the paths to a size of sixteen and abolishes the directionality restriction. With this setup, only 390 sentences do not get a path.\footnote{All sentence without a generated path get the artifical path \emph{NOPATH}}






\section{Experiments}
\subsection{Baselines}
\label{sec:3_baseline}
As described in section \ref{sec:argmine}, there is no task which is similar enough the one at hand which could be used as a baseline. Thus, two baselines using the obtained data were created. The first baseline, shown in table \ref{tbl:3stratifiedbaseline} and \ref{tbl:binmaj}, assigns all sentences to the class \texttt{NONE}.

% 24.3
\begin{table}[!htb]
	\begin{minipage}{.5\linewidth}
		\caption{Random (stratified) baseline for the three-label scenario.}
		\label{tbl:3stratifiedbaseline}
		\centering
		      
		\begin{tabularx}{0.97\linewidth}{Xrrrr}
			\toprule
			                & precision                    & recall                       & f1 score                     \\ \midrule 
			\texttt{BETTER} & 0.22 \scriptsize{$\pm$0.02} & 0.23 \scriptsize{$\pm$0.02} & 0.22 \scriptsize{$\pm$0.02} \\ 
			\texttt{WORSE}  & 0.10 \scriptsize{$\pm$0.02} & 0.08 \scriptsize{$\pm$0.02} & 0.09 \scriptsize{$\pm$0.02} \\ 
			\texttt{NONE}   & 0.74 \scriptsize{$\pm$0.00}  & 0.74 \scriptsize{$\pm$0.01} & 0.74 \scriptsize{$\pm$0.01} \\ 
			avg.         & 0.59 \scriptsize{$\pm$0.01} & 0.59 \scriptsize{$\pm$0.01} & 0.59 \scriptsize{$\pm$0.01} \\ 
			\bottomrule
		\end{tabularx} 
		
	\end{minipage}%
	\begin{minipage}{.5\linewidth}
		\centering
		\caption{Majority class baseline for the  for the three-label scenario.}
		\label{tbl:3majoritybaseline}
		\begin{tabularx}{0.97\linewidth}{Xrrrr}
			\toprule
			                & precision                    & recall                       & f1 score                                    \\ \midrule 
			\texttt{BETTER} & 0.00 \scriptsize{$\pm$0.00} & 0.00 \scriptsize{$\pm$0.00} & 0.00 \scriptsize{$\pm$0.00}                \\ 
			\texttt{WORSE}  & 0.00 \scriptsize{$\pm$0.00} & 0.00 \scriptsize{$\pm$0.00} & 0.00 \scriptsize{$\pm$0.00}                \\ 
			\texttt{NONE}   & 0.73 \scriptsize{$\pm$0.00}     & 1.00 \scriptsize{$\pm$0.00} & 0.84 \scriptsize{$\pm$0.00}                \\ 
			avg.         & 0.53 \scriptsize{$\pm$0.00} & 0.73 \scriptsize{$\pm$0.00} & \textbf{0.61} \scriptsize{$\pm$0.00} \\ 
			\bottomrule
		\end{tabularx}
	\end{minipage} 
\end{table}


The second baseline was created by assigning classes to the data at random, respecting the distribution of classes in the original data. The results are shown in tables \ref{tbl:3majoritybaseline} and \ref{tbl:binstrat}.



\begin{table}[!htb]
	\begin{minipage}{.5\linewidth}
		\caption{Random (stratified) baseline for the binary scenario.}
		\label{tbl:binmaj}
		\centering
		      
		\begin{tabularx}{0.97\linewidth}{Xrrrr}
			\toprule
			              & precision                    & recall                       & f1 score                              \\ \midrule 
			\texttt{ARG}  & 0.31 \scriptsize{$\pm$0.02} & 0.31 \scriptsize{$\pm$0.02} & 0.31 \scriptsize{$\pm$0.02}          \\ 
			\texttt{NONE} & 0.74 \scriptsize{$\pm$0.01} & 0.74 \scriptsize{$\pm$0.01} & 0.74 \scriptsize{$\pm$0.01}          \\ 
			avg.       & 0.62 \scriptsize{$\pm$0.01} & 0.62 \scriptsize{$\pm$0.01} & \textbf{0.62} \scriptsize{$\pm$0.01} \\ 
			\bottomrule
		\end{tabularx}
		
	\end{minipage}%
	\begin{minipage}{.5\linewidth}
		\centering
		\caption{Majority class baseline for the binary scenario.}
		\label{tbl:binstrat}
		\begin{tabularx}{0.97\linewidth}{Xrrrr}
			\toprule
			              & precision                    & recall                       & f1 score                     \\ \midrule 
			\texttt{ARG}  & 0.00 \scriptsize{$\pm$0.00} & 0.00 \scriptsize{$\pm$0.00} & 0.00 \scriptsize{$\pm$0.00} \\ 
			\texttt{NONE} & 0.73 \scriptsize{$\pm$0.00} & 1.00 \scriptsize{$\pm$0.00} & 0.84 \scriptsize{$\pm$0.00} \\ 
			avg.       & 0.53 \scriptsize{$\pm$0.00} & 0.73 \scriptsize{$\pm$0.00} & 0.61 \scriptsize{$\pm$0.00} \\ 
			\bottomrule
		\end{tabularx}
	\end{minipage} 
\end{table}
For all baselines, the scikit-learn's \texttt{DummyClassifer} was used.

\subsection{Results}
\label{sec:3_results}
The classification results for the best performing feature configurations in the three-class scenario are presented in figures \ref{fig:3_f1} (f1 score), \ref{fig:3_precision} (precision) and \ref{fig:3_recall} (recall). Each feature was tested and evaluated using five stratified folds. Presented are the overall results and the results for each class. The black bar shows the standard derivation. All scores were calculated with scikit-learn's metric module (\texttt{sklearn.metrics}). All features except the LexNet features used the middle part of the sentence and left the objects untouched. In the LexNet features, the objects were replaced with \emph{Objecta} and \emph{Objectb}. \emph{LexNet (original} used the full sentence and \emph{LexNet (customised} used the middle part of the sentence. 


\begin{figure}[htbp]
      \caption{\textbf{F1 score} for the three-class scenario using XGBoost. The gray bar shows the weighted average of each class. The feature name (see section \ref{sec:features}) is presented on the x-axis, the f1 score on the y-axis. The black bar displays the standard derivation.} 
    \label{fig:3_f1}
 \centering
	\includegraphics[width=1\textwidth]{images/experiments/f1-False}

\end{figure}



All features yield f1 scores at least ten points over the baseline. Bag-of-words (f1 score 0.855) and InferSent (f1 score 0.858) deliver almost identical results. The boolean feature that captures comparative adjectives in the middle of the sentence yields a f1 score over the baseline as well. However, it does not assign any examples to the class \texttt{WORSE}.

Despite the fact that only 1502 sentences got a path embedding for \emph{LexNet (original)}, the feature is able to predict some sentences correct. This indicates that this feature setup is reasonable and would work good if more examples were present. An experiment with only the 1502 sentences confirms this, as the feature is then able to achieve an f1 score of 0.76.\newline


\begin{figure}[htbp]
         \caption{\textbf{Precision} for the three-class scenario using XGBoost. The blue bar shows the weighted average of each class. The feature name (see section \ref{sec:features}) is presented on the x-axis, the precision score on the y-axis. The black bar displays the standard derivation.} 
    \label{fig:3_precision}
 \centering
	\includegraphics[width=1\textwidth]{images/experiments/precision-False}
\end{figure}

  \begin{figure}[htbp]
              \caption{\textbf{Recall} for the three-class scenario using XGBoost. The blue bar shows the weighted average of each class. The feature name (see section \ref{sec:features}) is presented on the x-axis, the recall score on the y-axis. The black bar displays the standard derivation.} 
       \label{fig:3_recall}
 \centering
	\includegraphics[width=1\textwidth]{images/experiments/recall-False}
\end{figure}

\FloatBarrier

% === binary 
Figures \ref{fig:2_f1} (f1 score), \ref{fig:2_precision} (precision) and \ref{fig:2_recall} (recall) show the results for the binary classification.
\begin{figure}[htbp]
      \caption{\textbf{F1 score} for the binary scenario using XGBoost. The blue bar shows the weighted average of each class. The feature name (see section \ref{sec:features}) is presented on the x-axis, the f1 score on the y-axis. The black bar displays the standard derivation.} 
    \label{fig:3_f1}

    \label{fig:2_f1}
 \centering
	\includegraphics[width=1\textwidth]{images/experiments/f1-True}

\end{figure}
As with the three-class scenario, InferSent (f1 score 0.894) and the bag-of-words (f1 score 0.890) performed best and got almost equal results. In contrast to the three-classes scenario, they are closely followed by mean word embeddings. In summary, all vector representations worked good and got f1 scores close to each other. The feature \emph{LexNet (original)} is again the worst, yet the score is nineteen points above the baseline.







\begin{figure}[htbp]
         \caption{\textbf{Precision} for the binary scenario using XGBoost. The blue bar shows the weighted average of each class. The feature name (see section \ref{sec:features}) is presented on the x-axis, the precision score on the y-axis. The black bar displays the standard derivation.} 
    \label{fig:2_precision}
    \centering
	\includegraphics[width=1\linewidth]{images/experiments/precision-True}
    \end{figure}
    
    \begin{figure}[tb]
              \caption{\textbf{Precision} for the binary scenario using XGBoost. The blue bar shows the weighted average of each class. The feature name (see section \ref{sec:features}) is presented on the x-axis, the binary score on the y-axis. The black bar displays the standard derivation.} 
       \label{fig:2_recall}
 \centering
	\includegraphics[width=1\linewidth]{images/experiments/recall-True}

\end{figure}



\FloatBarrier
\subsection{Error analysis}
\label{sec:error_analysis}
Figure \ref{fig:3_conf_inf} displays the confusion matrix for the best lexical feature in the three-class scenario (InferSent, also the best feature overall), while figure \ref{fig:3_conf_uni} shows the confusion matrix for the best syntax feature (customised LexNet).
 The confusion matrices of each fold per feature were summed up to create these tables.





\begin{figure}[h]
    \begin{minipage}{.5\linewidth}
   \caption{Confusion matrix for the InferSent feature using XGBoost} 
    \label{fig:3_conf_inf}
 \centering
	\includegraphics[width=1\linewidth]{images/experiments/conf-InferSent_False}
  \end{minipage} \hfill
    \begin{minipage}{.5\linewidth}
  
     \caption{Confusion matrix for the customised LexNet feature using XGBoost} 
       \label{fig:3_conf_uni}
 \centering
	\includegraphics[width=1\linewidth]{images/experiments/conf-middle_paths_unrestricted_16_False}
    \end{minipage} 
\end{figure}

As presented above, \texttt{WORSE} was the hardest class to recognize. The matrices show that it was more often confused with \texttt{NONE} than with \texttt{BETTER}. This is contrary to the expections. The classes  \texttt{BETTER} and \texttt{WORSE} are representing argumentative sentence. It was expected that the distinction between argumentative and not-argumentative is clearer. 

In total, 1274 sentences were incorrectly classied. Both features made the same errors on 570 sentences. The InferSent feature made 219 additional errors, while the LexNet feature made 485.  Examples are shown in table \ref{tbl:3_mistakes}. Suprisingly, the majority of errors was made on sentences with a high confidence. Four-hundred of the shared errors were made on sentences with a confidence of one. InferSent made 156 errors on highly confident sentences, while LexNet made 336.


Examples on errors made solely by the InferSent feature are given in table \ref{tbl:3_mistakes_se}.
\begin{table}[h]
\caption{Example sentences for errors made by the classifier in the three-label scenario with the InferSent feature. The objects of interest are printed \textbf{bold}. Confidence shows the confiedence of the annotators and is calculated as \emph{judgments for majority class / total judgments}.}
\label{tbl:3_mistakes_se}
\begin{tabularx}{\linewidth}{lXrrr}
\toprule
 & Sentence & Predicted & Gold & Confidence \\ \midrule
1& Is \textbf{football} better than \textbf{baseball}? & \texttt{BETTER} & \texttt{NONE} & 1.0\\
2& Is \textbf{Microsoft} better because of \textbf{Apple}? & \texttt{BETTER} & \texttt{NONE} & 1.0\\
 
3& The \textbf{Buick} is a far better driver but doesn't have the wretched excess appeal of the \textbf{Cadillac}. & \texttt{BETTER} & \texttt{NONE} & 1.0\\
4&They know that \textbf{Lexus} has better residual, and that \textbf{Audi} has better tech. & \texttt{BETTER} & \texttt{NONE} & 1.0\\
 
5& Its Azure PaaS/IaaS platform hasn't overtaken \textbf{Amazon} yet in market share, but \textbf{Microsoft} has enjoyed nine straight quarters of growth at 10 percent or better & \texttt{NONE} & \texttt{WORSE} & 1.0\\
 
 6& I think I found a better \textbf{sandwich} than a cheese \textbf{steak}. & \texttt{NONE} & \texttt{BETTER} & 1.0\\
 
7&  \textbf{harvard} College accepted just 6.9 percent, its lowest rate ever, and the numbers were not much better elsewhere: 7.2 percent at Stanford,8.2 percent at Princeton, 11.5 percent at \textbf{dartmouth}, and 23 percent at Northwestern-all record lows. & \texttt{WORSE} & \texttt{NONE} & 0.4\\
\bottomrule
\end{tabularx}
\end{table}

The first two sentences look comparative, but they are questions. As said in the guidelines, all questions should be labelled as \texttt{NONE}, but InferSent frequently classified questions as comparative. Sentences three and four are comparative, but it has no clear winner of the comparison. The guidelines formulated that only sentences with clear winners should be labelled with \texttt{BETTER} or \texttt{WORSE}. InferSent was not able to learn this restriction. Sentence seven is a hard sentence. Even the annotators were not able to settle on one class. The missclassification of sentence six comes with some surprise. It is assumed that the position of the cue word \emph{better} leads to the wrong class, as it is not between the two objects.

Table \ref{tbl:3_mistakes_lexnet} shows examples for errors exclusivly made by the LexNet feature. As described in section \ref{sec:lexnet_feat_desc}, 390 of the sentences did not get a dependency path. However, only fourty-one of the wrongly classified sentences do not have a path.

\begin{table}[h]
\caption{Example sentences for errors made by the classifier in the three-label scenario with the LexNet feature. The objects of interest are printed \textbf{bold}. Confidence shows the confidence of the annotators and is calculated as \emph{judgments for majority class / total judgments}.}
\label{tbl:3_mistakes_lexnet}
\begin{tabularx}{\linewidth}{lXrrr}
\toprule
 & Sentence & Predicted & Gold & Confidence \\ \midrule
1 & Right now \textbf{Apple} is worse then \textbf{Microsoft} ever was. & \texttt{BETTER} & \texttt{WORSE} & 1.0 \\
2 & I think \textbf{Bash} scripting is harder than \textbf{Python} though. & \texttt{BETTER} & \texttt{WORSE} & 1.0\\
3 & In my opinion \textbf{Windows 8} is worlds better then \textbf{Windows 7} & \texttt{NONE} & \texttt{BETTER} & 1.0\\
4 & In FPGAs, \textbf{Integer} or fixed-point math can often run 10 to 100 times faster than \textbf{Floating-point} 
computations. & \texttt{NONE} & \texttt{BETTER} & 1.0\\
5 & Congratulations to \textbf{Apple}; you continue to prove you are worse than \textbf{Microsoft} ever was. & \texttt{BETTER} & \texttt{WORSE} & 0.6\\
 \bottomrule
\end{tabularx}
\end{table}

It is salient that the LexNet feature makes errors an fairly simple sentences like the first one in table \ref{tbl:3_mistakes_lexnet}. While InferSent's errors can be coarsly grouped, the errors made by LexNet seem more random. It is assumed that the amount of training data for the neural network encoder is not big enough to create embeddings which are meaningful enough. However, the overall result of LexNet indicates that a encoder trained on more data will yield good results.


Sentences which were wrongly classified by both features are presented in table \ref{tbl:3_mistakes_both}. Both features predicted the same class for 457 of the 570 shared errors.


\begin{table}[h]
\caption{Example sentences for errors made by both features in the three-label scenario. The objects of interest are printed \textbf{bold}. \emph{Pred. IF} shows the predicted class of the InferSent feature, \emph{Pred. LexNet} the prediction of the LexNet feature. Confidence shows the confidence of the annotators and is calculated as \emph{judgments for majority class / total judgments}.}
\label{tbl:3_mistakes_both}
\begin{tabularx}{\linewidth}{lXrrrr}
\toprule
 & Sentence & Pred. IF & Pred. LexNet & Gold & Conf. \\ \midrule
1 & Is a \textbf{BMW} 3 series \$15,000 better than a \textbf{Ford} Focus? & \texttt{BETTER} & \texttt{BETTER} & \texttt{NONE} & 1.0\\
2 & \textbf{Google} is the main player now, \textbf{Microsoft} are just plain inferior in Mobile & \texttt{NONE} & \texttt{NONE} & \texttt{BETTER} & 1.0\\
3 & Yeah, Nvidia's \textbf{OpenCL} is not good and \textbf{CUDA} is way better. & \texttt{NONE} & \texttt{BETTER} & \texttt{WORSE} & 1.0\\
4 & \textbf{Python} has a better standard library, but \textbf{Perl} has a better nonstandard library & \texttt{BETTER} & \texttt{WORSE} & \texttt{NONE} & 0.8\\
5 & \textbf{Groovy} code often looks and feels like \textbf{Java} code, but is almost always simpler and easier to use. & \texttt{NONE} & \texttt{NONE} & \texttt{BETTER} & 0.4\\

 \bottomrule
\end{tabularx}
\end{table}

The shared errors are similar to the errors made exclusivly by InferSent. Again, they can be coarsly grouped, for instance into questions (sentence one in table \ref{tbl:3_mistakes_both}) or sentences without a clear winner (sentence four).\newline

In the binary scenario, 1144 errors were made. Both features made the same errors on 394 sentences. LexNet made 496 unique errors, while InferSent made 254. However, 671 errors were already made in the three-class scenario, so that only 473 errors are new. All in all, the analysis of the errors made in the binary scenario does not give any new insights. Again, the majority of errors was made on sentences with a high confidence. The errors made exclusivly by LexNet seem random again, while the shared errors and the errors made by InferSent have similar sources (e.g. questions, no clear winner, complex sentences).

It was expected that union of \texttt{BETTER} and \texttt{WORSE} into one class will improve the result significantally, as the class imbalance is reduced this way. However, this does not appear to be the case. The classifier made almost as many mistakes as in the three-class scenario.

\FloatBarrier
\section{Discussion}
Section \ref{sec:3_results} only shows the results for the best performing configuration of each feature. This is, for all cases, the middle part of the sentence. Intuitivly, this makes sense. Comparative sentences are often formed after the pattern \emph{Noun Verb Comparative Adjective Noun}, as in \emph{\enquote{Python is better than Ruby}}. Sentences which are not formed after this pattern caused wrong predictions, as presented in \ref{sec:error_analysis}.
 Using the full sentence worked second best. Adding the first and/or last part of the sentence did not increase the f1 score at all, no matter if the same or another represenation type than the one for the middle part is used. The first and second part alone never got an f1 score above the baseline.

Replacing or removing the objects did not increase the score significantly. In most cases, the difference in the f1 score between no replacement/removal and the best replacement/removal strategy was only reflected in the third or fourth decimal place. Hence, the concrete objects are not important at all for the classification. This is also supported by the fact that adding the word vectors of the objects as features did not increase the result for any vector representation.

All in all, no combination of vector representations increased the score in any way. It was expected that a combination of LexNet features and one of the other features like InferSent will increase the f1 score, as they encode different information (lexical and syntactial). Hower, this was not the case. Adding the LexNet vectors to the InferSent vectors reduced the scores.

An interesting observation is that the simple bag-of-words model performs almost equal to the more complex models. Even a single boolean feature which captures if the sentence contains a comparative adjective yields an f1 score way over the baseline.

As expected, the smallest class in the data set caused the biggest problems. The recall for \texttt{WORSE} is about three times below the recall for \texttt{BETTER}.
Precision, recall and f1 score of \texttt{WORSE} have a high standard derivation. Looking at the confusion matrices in figure \ref{fig:3_conf_uni} and \ref{fig:3_conf_inf}, \texttt{WORSE} was confused with \texttt{NONE} for the majority of cases. Intuitivly, a confusion between \texttt{WORSE} and \texttt{BETTER} was expected, since both classes should reflect special cases of a class \texttt{ARG}.

Another interesting observation is that the three-class scenario and the binary scenario made mistakes on the same sentences. In fact, the majority of mistakes made by the binary scenario were made by the three-classes scenario as well.


\section{Evaluation with the held-out data}
The final evaluation was done with 1455 sentences which were never seen by the model before.

\label{sec:final}

