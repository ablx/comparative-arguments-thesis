\chapter{Conclusion and future work}
%hypnet features might work \texttt{BETTER} with more examples -> sample from the %index
The thesis dealt with the problem of Comparative Argument Mining. 

The first part discussed the creation of a labelled data set which contains a wide range of comparative sentences.

The second part discussed how to create a machine learning system which is able to classify the sentences in the created data set. \emph{Gradient Boosted Decision Trees} turned out to be the best classifier for this task. Various simple (like bag-of-words) and complex features (like sentence embeddings) achieved f1 scores at least ten points over the baseline. As presented in section \ref{sec:3_results}, the f1 score was greatly increased by some preprocessing steps. It turned out that the words between the two compared objects are most important. Features calculated with only these words outperformed all features calculated with the whole sentence.

The simplification from a three-class problem to a binary problem (by merging the comparative classes \texttt{BETTER} and \texttt{WORSE} into one class \texttt{ARG}) increased the performance.

The final evaluation on unseen data showed that most features generalise well. All in all, the classification works satisfactory.
\newline


Some aspects were not covered in this thesis. As described in section \ref{sec:mainstudy}, the data set was created on the sentence level. Because of this, no context information is available for the classification. However, the context can hold important information. For instance, the presented system does not work with a sentence like \emph{\enquote{This is better than Java.}} because the second object is missing. The preceding sentence might contain the object which is referenced by \emph{\enquote{This}}. 

Section \ref{sec:3_results} showed that the features based on LexNet yield acceptable results. It is expected that the results would increase if more data is available to create the path embeddings. In \cite{DBLP:conf/acl/ShwartzGD16} and \cite{DBLP:journals/corr/ShwartzD16}, the systems were trained on a Wikipedia corpus, which is magnitudes larger than the 7199 sentences from the corpus created in this thesis. One (costly) approach for future work is to annotate more data. Another approach could sample new sentences from the index, by just using patterns like \emph{\enquote{is better than}} or \emph{\enquote{is worse than}}. The quality would not be so good as with manually labelled data, but ths might be compensated by the neural network if it is trained long enough.

The results in \ref{sec:final} show that several features generalise well. The f1 score for unseen data is comparable to the scores during the development phase. Yet, the system was not tested in a real world application. This could be a comparison search engine which takes two objects as the input and returns all comparisons. In a next step, the search engine could inspect the retrieved comparions and extract compared properties and the like.

\appendix

	\counterwithin{table}{section}
	\chapter{Detailed Classification Results}
\section{Feature Experiments}
	\setcounter{section}{1}
	The following shows the classification result for each feature. Each feature was tested with five stratified folds. The result is presented as the average out of five folds with standard derivation. The class \texttt{ARG} is the union of \texttt{BETTER} and \texttt{WORSE}.
	

	
	\begin{table}[h] 
		\centering 
		\caption{Bag-Of-Words feature (three-class scenario). The presence of all unigrams in the corpus are represented as binary features.} 
		\label{  }
		\begin{tabular}{@{}lrrrr@{}}
			\toprule
			        & precision                & recall                   & f1 score                 \\ \midrule 
			\texttt{BETTER}  & 0.80 \scriptsize{(0.02)} & 0.72 \scriptsize{(0.02)} & 0.75 \scriptsize{(0.01)} \\ 
			\texttt{WORSE}   & 0.62 \scriptsize{(0.06)} & 0.39 \scriptsize{(0.05)} & 0.48 \scriptsize{(0.05)} \\ 
			\texttt{NONE}    & 0.89 \scriptsize{(0.01)} & 0.95 \scriptsize{(0.01)} & 0.92 \scriptsize{(0.00)} \\ 
			average & 0.85 \scriptsize{(0.01)} & 0.86 \scriptsize{(0.00)} & 0.85 \scriptsize{(0.01)} \\ 
			\bottomrule
		\end{tabular}
	\end{table}
	
	
	\begin{table}[h] 
		\centering 
		\caption{Bag-Of-Words feature (binary scenario). The presence of all unigrams in the corpus are represented as binary features.} 
		\label{  }
		\begin{tabular}{@{}lrrrr@{}}
			\toprule
			        & precision                & recall                   & f1 score                 \\ \midrule 
			\texttt{ARG}     & 0.80 \scriptsize{(0.02)} & 0.74 \scriptsize{(0.01)} & 0.77 \scriptsize{(0.01)} \\ 
			\texttt{NONE}    & 0.91 \scriptsize{(0.00)} & 0.93 \scriptsize{(0.01)} & 0.92 \scriptsize{(0.00)} \\ 
			average & 0.88 \scriptsize{(0.00)} & 0.88 \scriptsize{(0.00)} & 0.88 \scriptsize{(0.00)} \\ 
			\bottomrule
		\end{tabular}
	\end{table}
	
	
	
	\begin{table}[h] 
		\centering 
		\caption{InferSent (sentence embeddings) feature (three-class scenario).} 
		\label{  }
		\begin{tabular}{@{}lrrrr@{}}
			\toprule
			        & precision                & recall                   & f1 score                 \\ \midrule 
			\texttt{BETTER}  & 0.79 \scriptsize{(0.02)} & 0.73 \scriptsize{(0.03)} & 0.76 \scriptsize{(0.02)} \\ 
			\texttt{WORSE}   & 0.53 \scriptsize{(0.07)} & 0.34 \scriptsize{(0.02)} & 0.41 \scriptsize{(0.03)} \\ 
			\texttt{NONE}    & 0.90 \scriptsize{(0.01)} & 0.95 \scriptsize{(0.00)} & 0.92 \scriptsize{(0.00)} \\ 
			average & 0.85 \scriptsize{(0.01)} & 0.86 \scriptsize{(0.01)} & 0.85 \scriptsize{(0.01)} \\ 
			\bottomrule
		\end{tabular}
	\end{table}
	
	\begin{table}[h] 
		\centering 
		\caption{InferSent (sentence embeddings) feature (binary scenario).} 
		\label{  }
		\begin{tabular}{@{}lrrrr@{}}
			\toprule
			        & precision                & recall                   & f1 score                 \\ \midrule 
			\texttt{ARG}     & 0.79 \scriptsize{(0.01)} & 0.78 \scriptsize{(0.02)} & 0.78 \scriptsize{(0.01)} \\ 
			\texttt{NONE}    & 0.92 \scriptsize{(0.01)} & 0.92 \scriptsize{(0.01)} & 0.92 \scriptsize{(0.00)} \\ 
			average & 0.88 \scriptsize{(0.01)} & 0.88 \scriptsize{(0.00)} & 0.88 \scriptsize{(0.00)} \\ 
			\bottomrule
		\end{tabular}
	\end{table}
	
	
	\begin{table}[h] 
		\centering 
		\caption{Mean Word Embeddings (three-class scenario). All GloVe word vectors of a sentence were summed up and divided by the number of words in the sentence.} 
		\label{  }
		\begin{tabular}{@{}lrrrr@{}}
			\toprule
			        & precision                & recall                   & f1 score                 \\ \midrule 
			\texttt{BETTER}  & 0.71 \scriptsize{(0.01)} & 0.73 \scriptsize{(0.04)} & 0.72 \scriptsize{(0.02)} \\ 
			\texttt{WORSE}   & 0.47 \scriptsize{(0.05)} & 0.16 \scriptsize{(0.02)} & 0.24 \scriptsize{(0.03)} \\ 
			\texttt{NONE}    & 0.88 \scriptsize{(0.01)} & 0.94 \scriptsize{(0.00)} & 0.91 \scriptsize{(0.00)} \\ 
			average & 0.82 \scriptsize{(0.01)} & 0.84 \scriptsize{(0.01)} & 0.82 \scriptsize{(0.01)} \\ 
			\bottomrule
		\end{tabular}
	\end{table}
	
	\begin{table}[h] 
		\centering 
		\caption{Mean Word Embeddings (binary class scenario). All GloVe word vectors of a sentence were summed up and divided by the number of words in the sentence.} 
		\label{  }
		\begin{tabular}{@{}lrrrr@{}}
			\toprule
			        & precision                & recall                   & f1 score                 \\ \midrule 
			\texttt{ARG}     & 0.78 \scriptsize{(0.01)} & 0.74 \scriptsize{(0.01)} & 0.76 \scriptsize{(0.01)} \\ 
			\texttt{NONE}    & 0.91 \scriptsize{(0.00)} & 0.92 \scriptsize{(0.00)} & 0.91 \scriptsize{(0.00)} \\ 
			average & 0.87 \scriptsize{(0.01)} & 0.88 \scriptsize{(0.01)} & 0.87 \scriptsize{(0.01)} \\ 
			\bottomrule
		\end{tabular}
	\end{table}
	

	\begin{table}[h] 
		\centering 
		\caption{N-gram POS feature (three-class scenario). The presence of the 500 most frequent part-of-speech bi-, tri- and four-grams were represented as binary features.} 
		\label{  }
		\begin{tabular}{@{}lrrrr@{}}
			\toprule
			        & precision                & recall                   & f1 score                 \\ \midrule 
			\texttt{BETTER}  & 0.57 \scriptsize{(0.01)} & 0.59 \scriptsize{(0.03)} & 0.58 \scriptsize{(0.02)} \\ 
			\texttt{WORSE}   & 0.26 \scriptsize{(0.02)} & 0.11 \scriptsize{(0.02)} & 0.15 \scriptsize{(0.02)} \\ 
			\texttt{NONE}    & 0.87 \scriptsize{(0.01)} & 0.92 \scriptsize{(0.02)} & 0.89 \scriptsize{(0.01)} \\ 
			average & 0.76 \scriptsize{(0.01)} & 0.79 \scriptsize{(0.01)} & 0.77 \scriptsize{(0.01)} \\ 
			\bottomrule
		\end{tabular}
	\end{table}
	
	\begin{table}[h] 
		\centering 
		\caption{N-gram POS feature (three-class scenario). The presence of the 500 most frequent part-of-speech bi-, tri- and four-grams were represented as binary features.} 
		\label{  }
		\begin{tabular}{@{}lrrrr@{}}
			\toprule
			        & precision                & recall                   & f1 score                 \\ \midrule 
			\texttt{ARG}     & 0.70 \scriptsize{(0.02)} & 0.69 \scriptsize{(0.01)} & 0.70 \scriptsize{(0.01)} \\ 
			\texttt{NONE}    & 0.89 \scriptsize{(0.00)} & 0.89 \scriptsize{(0.01)} & 0.89 \scriptsize{(0.00)} \\ 
			average & 0.84 \scriptsize{(0.00)} & 0.84 \scriptsize{(0.01)} & 0.84 \scriptsize{(0.00)} \\ 
			\bottomrule
		\end{tabular}
	\end{table}
	

	

	\begin{table}[h] 
		\centering 
		\caption{Binary feature which represents the presence of a comparative adjective in the sentence (three-class scenario).} 
		\label{ }
		\begin{tabular}{@{}lrrrr@{}}
			\toprule
			        & precision                & recall                   & f1 score                 \\ \midrule 
			\texttt{BETTER}  & 0.56 \scriptsize{(0.01)} & 0.62 \scriptsize{(0.04)} & 0.59 \scriptsize{(0.02)} \\ 
			\texttt{WORSE}   & 0.00 \scriptsize{(0.00)} & 0.00 \scriptsize{(0.00)} & 0.00 \scriptsize{(0.00)} \\ 
			\texttt{NONE}    & 0.85 \scriptsize{(0.01)} & 0.92 \scriptsize{(0.01)} & 0.88 \scriptsize{(0.01)} \\ 
			average & 0.73 \scriptsize{(0.01)} & 0.79 \scriptsize{(0.01)} & 0.76 \scriptsize{(0.01)} \\ 
			\bottomrule
		\end{tabular}
	\end{table}
	
		\begin{table}[h] 
		\centering 
		\caption{ Binary feature which represents the presence of a comparative adjective in the sentence (three-class scenario). } 
		\label{  }
		\begin{tabular}{@{}lrrrr@{}}
			\toprule
			        & precision                & recall                   & f1 score                 \\ \midrule 
			\texttt{ARG}     & 0.75 \scriptsize{(0.02)} & 0.53 \scriptsize{(0.02)} & 0.62 \scriptsize{(0.02)} \\ 
			\texttt{NONE}    & 0.84 \scriptsize{(0.01)} & 0.93 \scriptsize{(0.01)} & 0.89 \scriptsize{(0.01)} \\ 
			average & 0.82 \scriptsize{(0.01)} & 0.82 \scriptsize{(0.01)} & 0.81 \scriptsize{(0.01)} \\ 
			\bottomrule
		\end{tabular}
	\end{table}
	
	
	
	\begin{table}[h] 
		\centering 
		\caption{LexNet path embeddings with a maximum length of four and restrictions of the edge direction (three-class scenario). This setup is equal to the original setup in \cite{DBLP:journals/corr/ShwartzD16}} 
		\label{  }
		\begin{tabular}{@{}lrrrr@{}}
			\toprule
			        & precision                & recall                   & f1 score                 \\ \midrule 
			\texttt{BETTER}  & 0.65 \scriptsize{(0.02)} & 0.20 \scriptsize{(0.01)} & 0.30 \scriptsize{(0.02)} \\ 
			\texttt{WORSE}   & 0.58 \scriptsize{(0.12)} & 0.07 \scriptsize{(0.02)} & 0.13 \scriptsize{(0.03)} \\ 
			\texttt{NONE}    & 0.76 \scriptsize{(0.00)} & 0.98 \scriptsize{(0.00)} & 0.86 \scriptsize{(0.00)} \\ 
			average & 0.73 \scriptsize{(0.01)} & 0.76 \scriptsize{(0.00)} & 0.69 \scriptsize{(0.00)} \\ 
			\bottomrule
		\end{tabular}
	\end{table}
	
	\begin{table}[h] 
		\centering 
		\caption{LexNet path embeddings with a maximum length of four and restrictions of the edge direction (binary scenario). This setup is equal to the original setup in \cite{DBLP:journals/corr/ShwartzD16}} 
		\label{  }
		\begin{tabular}{@{}lrrrr@{}}
			\toprule
			        & precision                & recall                   & f1 score                 \\ \midrule 
			\texttt{ARG}     & 0.78 \scriptsize{(0.06)} & 0.21 \scriptsize{(0.02)} & 0.33 \scriptsize{(0.02)} \\ 
			\texttt{NONE}    & 0.77 \scriptsize{(0.00)} & 0.98 \scriptsize{(0.01)} & 0.86 \scriptsize{(0.01)} \\ 
			average & 0.77 \scriptsize{(0.02)} & 0.77 \scriptsize{(0.01)} & 0.72 \scriptsize{(0.01)} \\ 
			\bottomrule
		\end{tabular}
	\end{table}
	
	%===
	\begin{table}[h] 
		\centering 
		\caption{LexNet path embeddings with a maximum length of sixteen and no restrictions of the edge direction (three-class scenario).} 
		\label{  }
		\begin{tabular}{@{}lrrrr@{}}
			\toprule
			        & precision                & recall                   & f1 score                 \\ \midrule 
			\texttt{BETTER}  & 0.64 \scriptsize{(0.05)} & 0.64 \scriptsize{(0.03)} & 0.64 \scriptsize{(0.02)} \\ 
			\texttt{WORSE}   & 0.41 \scriptsize{(0.08)} & 0.15 \scriptsize{(0.04)} & 0.22 \scriptsize{(0.04)} \\ 
			\texttt{NONE}    & 0.87 \scriptsize{(0.01)} & 0.93 \scriptsize{(0.01)} & 0.90 \scriptsize{(0.00)} \\ 
			average & 0.79 \scriptsize{(0.01)} & 0.81 \scriptsize{(0.01)} & 0.80 \scriptsize{(0.01)} \\ 
			\bottomrule
		\end{tabular}
	\end{table}
	
	\begin{table}[h] 
		\centering 
		\caption{LexNet path embeddings with a maximum length of sixteen and no restrictions of the edge direction (binary scenario).} 
		\label{  }
		\begin{tabular}{@{}lrrrr@{}}
			\toprule
			        & precision                & recall                   & f1 score                 \\ \midrule 
			\texttt{ARG}     & 0.75 \scriptsize{(0.01)} & 0.65 \scriptsize{(0.02)} & 0.70 \scriptsize{(0.01)} \\ 
			\texttt{NONE}    & 0.88 \scriptsize{(0.00)} & 0.92 \scriptsize{(0.00)} & 0.90 \scriptsize{(0.00)} \\ 
			average & 0.84 \scriptsize{(0.00)} & 0.85 \scriptsize{(0.00)} & 0.84 \scriptsize{(0.00)} \\ 
			\bottomrule
		\end{tabular}
	\end{table}
	
\section{Final Held-Out Experiments}
