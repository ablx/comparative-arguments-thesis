{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mirco/Documents/comparative-arguments/ve/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"6133c7d1-9b63-4f1c-b815-58b44c333585\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"6133c7d1-9b63-4f1c-b815-58b44c333585\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"6133c7d1-9b63-4f1c-b815-58b44c333585\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '6133c7d1-9b63-4f1c-b815-58b44c333585' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.15.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"6133c7d1-9b63-4f1c-b815-58b44c333585\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"6133c7d1-9b63-4f1c-b815-58b44c333585\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"6133c7d1-9b63-4f1c-b815-58b44c333585\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '6133c7d1-9b63-4f1c-b815-58b44c333585' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.15.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"6133c7d1-9b63-4f1c-b815-58b44c333585\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras import Model\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure, output_file, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "from keras.layers import Dense, LSTM, Dropout, Embedding,Flatten,Bidirectional, MaxPooling1D,GlobalAveragePooling1D, GlobalMaxPooling1D, AveragePooling1D,Conv1D\n",
    "from keras.models import Sequential\n",
    "from collections import defaultdict\n",
    "from keras import regularizers\n",
    "from pprint import pprint\n",
    "from textwrap import wrap\n",
    "import json\n",
    "from keras import optimizers\n",
    "output_notebook()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= 50000\n",
    "maxlen = 20\n",
    "output_name_prefix = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path_file, data_file, remove_no_path=False):\n",
    "    frame = pd.read_csv(path_file)\n",
    "    if remove_no_path:\n",
    "        frame = frame[(frame.path != 'NOPATH')]\n",
    "    #frame = frame[frame.most_frequent_percentage >= 0.6]\n",
    "    data_frame = pd.read_csv(data_file)\n",
    "    data_frame = data_frame[data_frame.sentence.isin(frame.sentence.values.tolist())]\n",
    "    print('Paths: {} | Data: {}'.format(len(frame),len(data_frame)))\n",
    "    return frame, data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path_frame,reshaped=False):\n",
    "    # transform labels into 3d tensors for lstm\n",
    "    paths_list = path_frame['path'].values.tolist()\n",
    "    tokenizer = Tokenizer(num_words=vocab_size,filters='', split=' ')\n",
    "    tokenizer.fit_on_texts(paths_list)\n",
    "    input_train = tokenizer.texts_to_sequences(paths_list)\n",
    "    input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "\n",
    "    # create target labels, one for each path\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    label_encoder.fit(paths_list)\n",
    "    y_train =  to_categorical(label_encoder.transform(paths_list))\n",
    "    if reshaped:\n",
    "        input_train =  input_train.reshape((len(paths_list),maxlen,1))\n",
    "    pprint('Tokens {}'.format(len(tokenizer.word_counts)))\n",
    "    print('{} paths total'.format(len(paths_list)))\n",
    "    print('{} different (target) paths'.format(y_train[0].shape[0]))\n",
    "    return input_train, y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lines(history, first,second):\n",
    "    plt.plot(history.history[first[0]],color=first[1])\n",
    "    plt.plot(history.history[second[0]],linestyle='--',color=second[1])\n",
    "    plt.title('{} vs {}'.format(first[0],second[0]))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend([first[0],second[0]])\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(layers,X,y,name='',epochs=100,batch_size=64, shuffle=True, validation_split=0.4, optimizer='rmsprop',\n",
    "               activation_last_layer='softmax'):\n",
    "    model = Sequential(name=name)\n",
    "    [model.add(layer) for layer in layers]\n",
    "    model.add(Dense(y[0].shape[0],activation=activation_last_layer))\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    model.summary()\n",
    "    history = model.fit(X, y,\n",
    "    epochs=epochs, batch_size=batch_size,shuffle=shuffle, validation_split=validation_split)\n",
    "    return history, model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model,X):\n",
    "    layer = list(filter(lambda x: x.name == 'embedding',model.layers))[0]\n",
    "    print(layer)\n",
    "    assert layer.name == 'embedding'\n",
    "    intermediate_layer_model = Model(inputs=model.input,\n",
    "                                     outputs=layer.output)\n",
    "    return intermediate_layer_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_embeddings(embeddings,path_frame,data_frame):\n",
    "    assert len(path_frame) == len(embeddings)\n",
    "    embedding_dict = defaultdict(list)\n",
    "    mean_embedding_dict = {}\n",
    "    idx = 0\n",
    "    for i, row in path_frame.iterrows():\n",
    "        embedding_dict[row['sentence']].append(embeddings[idx])\n",
    "        idx += 1\n",
    "    print(len(embedding_dict), len(data_frame))\n",
    "    assert len(embedding_dict)== len(data_frame)\n",
    "\n",
    "    for k,v in embedding_dict.items():\n",
    "        adder = np.zeros(v[0].shape)\n",
    "        for val in v:\n",
    "            adder += val\n",
    "        adder /= len(v)\n",
    "        mean_embedding_dict[k] = adder\n",
    "\n",
    "    assert len(mean_embedding_dict)== len(data_frame)\n",
    "    return mean_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat(mean_embedding_dict,path_frame,data_frame):\n",
    "    X = []\n",
    "    y = []\n",
    "    plot_data = []\n",
    "    plot_x = []\n",
    "    idx = 0\n",
    "    embedding_df = pd.DataFrame(columns=['id', 'sentence', 'label', 'embedding', 'paths'])\n",
    "    for k,v in mean_embedding_dict.items():\n",
    "        f_slice = path_frame[path_frame.sentence == k]\n",
    "        label = f_slice.most_frequent_label.values.tolist()[0]\n",
    "        f_paths = f_slice.path.values.tolist()\n",
    "        embedding_df.loc[idx] = [f_slice.id.values.tolist()[0], wrap(f_slice.sentence.values.tolist()[0]),label,\n",
    "                                 v.reshape(-1,1).squeeze().tolist(), [wrap(p) for p in f_paths]]\n",
    "        idx+=1\n",
    "        # prepare results for classification\n",
    "        X.append(v.reshape(-1,1).squeeze().tolist())\n",
    "        y.append(label)\n",
    "\n",
    "        # prepare results for t-sne plot\n",
    "        plot_x.append(v)\n",
    "        plot_data.append( (label,k,'\\n'.join(f_paths)) )\n",
    "    return X, y, plot_data, plot_x, embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding_df(embedding_df,name):\n",
    "    embedding_df.to_csv(name+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(plot_x, plot_data):\n",
    "    X_embedded = TSNE(n_components=2,verbose=1).fit_transform(plot_x)\n",
    "    plot_frame = pd.DataFrame(columns=['x','y','class','sentence', 'path'])\n",
    "    for i,x in enumerate(X_embedded):\n",
    "        plot_frame.loc[i] = [x[0],x[1],plot_data[i][0],plot_data[i][1],plot_data[i][2]]\n",
    "\n",
    "    def build_source(label, df):\n",
    "        df = df[df['class'] == label]\n",
    "        return ColumnDataSource(data=dict(\n",
    "        x=df.x.values.tolist(),\n",
    "        y=df.y.values.tolist(),\n",
    "        sentence=df.sentence.tolist(),\n",
    "        path=df.path.tolist(),\n",
    "        label=df['class'].tolist()))\n",
    "\n",
    "    colormap = {'WORSE': 'red', 'BETTER': 'green', 'NONE': 'blue'}\n",
    "    colors = [colormap[x] for x in plot_frame['class']]\n",
    "\n",
    "    # GROUP tooltips\n",
    "    hover = HoverTool(tooltips=[\n",
    "        (\"Sentence\", \"@sentence\"),\n",
    "        (\"Path\", \"@path\"),\n",
    "        (\"label\", \"@label\"),\n",
    "    ])\n",
    "\n",
    "    p = figure(plot_width=900,  plot_height=900, tools=\"pan,wheel_zoom,box_zoom,reset,previewsave\")\n",
    "    p.add_tools(hover)\n",
    "\n",
    "    p.cross(x='x', y='y', source=build_source('NONE', plot_frame),size=5,color='gray')\n",
    "    p.circle(x='x', y='y', source=build_source('WORSE', plot_frame) ,size=5,color='red')\n",
    "    p.circle(x='x', y='y', source=build_source('BETTER', plot_frame),size=5,color='green')\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_test(X,y,data_frame,with_embeddings=False):\n",
    "    _X = X\n",
    "    if with_embeddings:\n",
    "        print(\"With embeddings\")\n",
    "        lst_lst = np.array([json.loads(l) for l in data_frame.embedding_middle_part.values.tolist()])\n",
    "        paths_ = np.asarray(X)\n",
    "        infersent_ = np.array(lst_lst)\n",
    "        _X = np.concatenate([paths_, infersent_],axis=1)\n",
    "        assert np.asarray(X).shape[0] == paths_.shape[0] == infersent_.shape[0]\n",
    "        assert _X.shape[1] == paths_.shape[1] + infersent_.shape[1]\n",
    "        \n",
    "    else:\n",
    "        print(\"Without embeddings\")\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=2, random_state=42)\n",
    "    \n",
    "    d = defaultdict(int)\n",
    "\n",
    "    for train_index, test_index in kf.split(_X,y):\n",
    "        X_train, X_test = np.array(_X)[train_index], np.array(_X)[test_index]\n",
    "        y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "     \n",
    "        log = LogisticRegression()\n",
    "        log.fit(X_train, y_train)\n",
    "        pred = log.predict(X_test)\n",
    "        print(classification_report(y_test,pred,labels=['BETTER', 'WORSE', 'NONE']))\n",
    "        print('=========\\n')\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_run(layers,path_file,\n",
    "             name='',reshape=False,epochs=100,batch_size=64, shuffle=True, validation_split=0.2, optimizer='rmsprop'\n",
    "             ,activation_last_layer='softmax'):\n",
    "    start = time.mktime(time.localtime())\n",
    "    path_frame, data_frame = load_data(path_file,  '../code/data/data.csv', remove_no_path=False)\n",
    "    \n",
    "    \n",
    "    X, y = preprocess(path_frame,reshaped=reshape)\n",
    "    history, trained_model = train_model(layers,X,y, name=name,epochs=epochs,batch_size=batch_size,\n",
    "                                         shuffle=shuffle,activation_last_layer=activation_last_layer, validation_split=validation_split, optimizer=optimizer)\n",
    "\n",
    "    plot_lines(history,('acc','green'),('val_acc', 'yellowgreen'))\n",
    "    plot_lines(history,('loss', 'red'), ('val_loss', 'orangered'))\n",
    "    embeddings = get_embeddings(trained_model,X)\n",
    "    mean_embedding_dict = average_embeddings(embeddings,path_frame,data_frame)\n",
    "    _X, _y, plot_data, plot_x, embedding_df = reformat(mean_embedding_dict,path_frame,data_frame)\n",
    "    #classification_test(_X,_y,data_frame)\n",
    "    #classification_test(_X,_y,data_frame,with_embeddings=True)\n",
    "    #save_embedding_df(embedding_df,name)\n",
    "    #show(tsne_plot(plot_x, plot_data))\n",
    "    \n",
    "    duration = (time.mktime(time.localtime()) - start) / 60\n",
    "    print('Took {} minutes'.format(duration))\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== middle_paths_unrestricted_16\n",
      "Paths: 10067 | Data: 5759\n",
      "'Tokens 3760'\n",
      "10067 paths total\n",
      "4348 different (target) paths\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 100)           5000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 200)           240800    \n",
      "_________________________________________________________________\n",
      "embedding (GlobalMaxPooling1 (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4348)              873948    \n",
      "=================================================================\n",
      "Total params: 6,114,748\n",
      "Trainable params: 6,114,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8053 samples, validate on 2014 samples\n",
      "Epoch 1/150\n",
      "8053/8053 [==============================] - 30s 4ms/step - loss: 6.5383 - acc: 0.2032 - val_loss: 6.2140 - val_acc: 0.2264\n",
      "Epoch 2/150\n",
      "8053/8053 [==============================] - 26s 3ms/step - loss: 5.8525 - acc: 0.2068 - val_loss: 6.5104 - val_acc: 0.2264\n",
      "Epoch 3/150\n",
      "8053/8053 [==============================] - 27s 3ms/step - loss: 5.5302 - acc: 0.2076 - val_loss: 6.0223 - val_acc: 0.2264\n",
      "Epoch 4/150\n",
      "8053/8053 [==============================] - 25s 3ms/step - loss: 5.2158 - acc: 0.2778 - val_loss: 5.9579 - val_acc: 0.3615\n",
      "Epoch 5/150\n",
      "8053/8053 [==============================] - 25s 3ms/step - loss: 4.9308 - acc: 0.3619 - val_loss: 5.7116 - val_acc: 0.3982\n",
      "Epoch 6/150\n",
      "8053/8053 [==============================] - 25s 3ms/step - loss: 4.6539 - acc: 0.3886 - val_loss: 5.3898 - val_acc: 0.4280\n",
      "Epoch 7/150\n",
      "8053/8053 [==============================] - 25s 3ms/step - loss: 4.4323 - acc: 0.4125 - val_loss: 5.2544 - val_acc: 0.4280\n",
      "Epoch 8/150\n",
      "8053/8053 [==============================] - 26s 3ms/step - loss: 4.2613 - acc: 0.4195 - val_loss: 5.2457 - val_acc: 0.4419\n",
      "Epoch 9/150\n",
      "8053/8053 [==============================] - 25s 3ms/step - loss: 4.1179 - acc: 0.4253 - val_loss: 5.1745 - val_acc: 0.4513\n",
      "Epoch 10/150\n",
      "8053/8053 [==============================] - 25s 3ms/step - loss: 3.9889 - acc: 0.4294 - val_loss: 4.8531 - val_acc: 0.4598\n",
      "Epoch 11/150\n",
      "8053/8053 [==============================] - 25s 3ms/step - loss: 3.8767 - acc: 0.4354 - val_loss: 5.1774 - val_acc: 0.4623\n",
      "Epoch 12/150\n",
      "8053/8053 [==============================] - 25s 3ms/step - loss: 3.7636 - acc: 0.4465 - val_loss: 4.8316 - val_acc: 0.4727\n",
      "Epoch 13/150\n",
      "8053/8053 [==============================] - 26s 3ms/step - loss: 3.6497 - acc: 0.4591 - val_loss: 5.0247 - val_acc: 0.4816\n",
      "Epoch 14/150\n",
      "8053/8053 [==============================] - 25s 3ms/step - loss: 3.5453 - acc: 0.4673 - val_loss: 4.8867 - val_acc: 0.4911\n",
      "Epoch 15/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 3.4528 - acc: 0.4763 - val_loss: 4.9221 - val_acc: 0.4955\n",
      "Epoch 16/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 3.3694 - acc: 0.4833 - val_loss: 4.8329 - val_acc: 0.5045\n",
      "Epoch 17/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 3.2862 - acc: 0.4912 - val_loss: 5.3568 - val_acc: 0.5084\n",
      "Epoch 18/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 3.2083 - acc: 0.4947 - val_loss: 5.4691 - val_acc: 0.5119\n",
      "Epoch 19/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 3.1373 - acc: 0.5004 - val_loss: 4.8151 - val_acc: 0.5134\n",
      "Epoch 20/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 3.0595 - acc: 0.5057 - val_loss: 5.5677 - val_acc: 0.5164\n",
      "Epoch 21/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 2.9886 - acc: 0.5106 - val_loss: 5.2803 - val_acc: 0.5233\n",
      "Epoch 22/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 2.9194 - acc: 0.5191 - val_loss: 5.6000 - val_acc: 0.5323\n",
      "Epoch 23/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 2.8476 - acc: 0.5260 - val_loss: 5.4738 - val_acc: 0.5357\n",
      "Epoch 24/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 2.7742 - acc: 0.5358 - val_loss: 5.7827 - val_acc: 0.5412\n",
      "Epoch 25/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 2.7005 - acc: 0.5412 - val_loss: 5.3418 - val_acc: 0.5422\n",
      "Epoch 26/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 2.6307 - acc: 0.5477 - val_loss: 5.5469 - val_acc: 0.5487\n",
      "Epoch 27/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 2.5507 - acc: 0.5613 - val_loss: 5.8612 - val_acc: 0.5526\n",
      "Epoch 28/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 2.4678 - acc: 0.5675 - val_loss: 5.9538 - val_acc: 0.5511\n",
      "Epoch 29/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 2.3951 - acc: 0.5800 - val_loss: 6.0322 - val_acc: 0.5556\n",
      "Epoch 30/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 2.3301 - acc: 0.5866 - val_loss: 6.0094 - val_acc: 0.5551\n",
      "Epoch 31/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 2.2560 - acc: 0.5977 - val_loss: 5.8541 - val_acc: 0.5586\n",
      "Epoch 32/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 2.1955 - acc: 0.6091 - val_loss: 6.1471 - val_acc: 0.5591\n",
      "Epoch 33/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 2.1280 - acc: 0.6183 - val_loss: 5.9709 - val_acc: 0.5616\n",
      "Epoch 34/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 2.0619 - acc: 0.6306 - val_loss: 6.0091 - val_acc: 0.5650\n",
      "Epoch 35/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 1.9935 - acc: 0.6431 - val_loss: 6.1391 - val_acc: 0.5685\n",
      "Epoch 36/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 1.9281 - acc: 0.6540 - val_loss: 6.2020 - val_acc: 0.5680\n",
      "Epoch 37/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 1.8639 - acc: 0.6687 - val_loss: 6.2033 - val_acc: 0.5700\n",
      "Epoch 38/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 1.8000 - acc: 0.6817 - val_loss: 5.9924 - val_acc: 0.5755\n",
      "Epoch 39/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 1.7447 - acc: 0.6954 - val_loss: 6.0850 - val_acc: 0.5760\n",
      "Epoch 40/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 1.6763 - acc: 0.7089 - val_loss: 6.0646 - val_acc: 0.5770\n",
      "Epoch 41/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 1.6248 - acc: 0.7182 - val_loss: 6.0871 - val_acc: 0.5789\n",
      "Epoch 42/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 1.5636 - acc: 0.7302 - val_loss: 6.1334 - val_acc: 0.5785\n",
      "Epoch 43/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 1.5127 - acc: 0.7437 - val_loss: 6.0839 - val_acc: 0.5824\n",
      "Epoch 44/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 1.4548 - acc: 0.7565 - val_loss: 6.1409 - val_acc: 0.5844\n",
      "Epoch 45/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 1.4046 - acc: 0.7651 - val_loss: 6.1140 - val_acc: 0.5894\n",
      "Epoch 46/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 1.3409 - acc: 0.7806 - val_loss: 6.0665 - val_acc: 0.5879\n",
      "Epoch 47/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 1.2904 - acc: 0.7887 - val_loss: 6.0694 - val_acc: 0.5914\n",
      "Epoch 48/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 1.2379 - acc: 0.8016 - val_loss: 6.0997 - val_acc: 0.5963\n",
      "Epoch 49/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 1.1916 - acc: 0.8136 - val_loss: 5.9875 - val_acc: 0.5963\n",
      "Epoch 50/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 1.1319 - acc: 0.8206 - val_loss: 5.9130 - val_acc: 0.5983\n",
      "Epoch 51/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 1.0900 - acc: 0.8319 - val_loss: 5.9716 - val_acc: 0.5988\n",
      "Epoch 52/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 1.0382 - acc: 0.8407 - val_loss: 6.0106 - val_acc: 0.5988\n",
      "Epoch 53/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.9942 - acc: 0.8538 - val_loss: 6.0675 - val_acc: 0.6003\n",
      "Epoch 54/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.9512 - acc: 0.8627 - val_loss: 6.0197 - val_acc: 0.6023\n",
      "Epoch 55/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.9127 - acc: 0.8674 - val_loss: 6.0218 - val_acc: 0.6013\n",
      "Epoch 56/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.8735 - acc: 0.8779 - val_loss: 6.1094 - val_acc: 0.6013\n",
      "Epoch 57/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.8338 - acc: 0.8823 - val_loss: 5.9640 - val_acc: 0.6043\n",
      "Epoch 58/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.7959 - acc: 0.8930 - val_loss: 6.0165 - val_acc: 0.6053\n",
      "Epoch 59/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.7592 - acc: 0.9021 - val_loss: 6.0079 - val_acc: 0.6043\n",
      "Epoch 60/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.7227 - acc: 0.9067 - val_loss: 6.0449 - val_acc: 0.6072\n",
      "Epoch 61/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.6916 - acc: 0.9157 - val_loss: 6.0085 - val_acc: 0.6053\n",
      "Epoch 62/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.6548 - acc: 0.9213 - val_loss: 5.9550 - val_acc: 0.6058\n",
      "Epoch 63/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.6243 - acc: 0.9285 - val_loss: 5.9594 - val_acc: 0.6072\n",
      "Epoch 64/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.5974 - acc: 0.9332 - val_loss: 6.0036 - val_acc: 0.6097\n",
      "Epoch 65/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.5649 - acc: 0.9416 - val_loss: 5.9561 - val_acc: 0.6068\n",
      "Epoch 66/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.5366 - acc: 0.9456 - val_loss: 5.9101 - val_acc: 0.6107\n",
      "Epoch 67/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.5076 - acc: 0.9503 - val_loss: 5.9291 - val_acc: 0.6092\n",
      "Epoch 68/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.4927 - acc: 0.9537 - val_loss: 5.9121 - val_acc: 0.6107\n",
      "Epoch 69/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.4626 - acc: 0.9604 - val_loss: 5.8950 - val_acc: 0.6122\n",
      "Epoch 70/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.4428 - acc: 0.9636 - val_loss: 5.9253 - val_acc: 0.6107\n",
      "Epoch 71/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.4204 - acc: 0.9688 - val_loss: 5.9789 - val_acc: 0.6107\n",
      "Epoch 72/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.3922 - acc: 0.9711 - val_loss: 5.8969 - val_acc: 0.6122\n",
      "Epoch 73/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.3737 - acc: 0.9757 - val_loss: 5.9125 - val_acc: 0.6122\n",
      "Epoch 74/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.3504 - acc: 0.9786 - val_loss: 5.9300 - val_acc: 0.6122\n",
      "Epoch 75/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.3343 - acc: 0.9810 - val_loss: 5.9294 - val_acc: 0.6127\n",
      "Epoch 76/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.3171 - acc: 0.9824 - val_loss: 5.9375 - val_acc: 0.6127\n",
      "Epoch 77/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.2999 - acc: 0.9860 - val_loss: 5.9445 - val_acc: 0.6127\n",
      "Epoch 78/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.2798 - acc: 0.9888 - val_loss: 5.9183 - val_acc: 0.6122\n",
      "Epoch 79/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.2657 - acc: 0.9902 - val_loss: 5.9473 - val_acc: 0.6117\n",
      "Epoch 80/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.2518 - acc: 0.9918 - val_loss: 5.9907 - val_acc: 0.6122\n",
      "Epoch 81/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.2360 - acc: 0.9929 - val_loss: 5.8606 - val_acc: 0.6127\n",
      "Epoch 82/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.2229 - acc: 0.9934 - val_loss: 5.8425 - val_acc: 0.6117\n",
      "Epoch 83/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.2092 - acc: 0.9959 - val_loss: 5.8406 - val_acc: 0.6122\n",
      "Epoch 84/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.1958 - acc: 0.9962 - val_loss: 5.7835 - val_acc: 0.6127\n",
      "Epoch 85/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.1858 - acc: 0.9965 - val_loss: 5.8522 - val_acc: 0.6127\n",
      "Epoch 86/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.1750 - acc: 0.9973 - val_loss: 5.8780 - val_acc: 0.6127\n",
      "Epoch 87/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.1644 - acc: 0.9979 - val_loss: 5.8467 - val_acc: 0.6127\n",
      "Epoch 88/150\n",
      "8053/8053 [==============================] - 28s 3ms/step - loss: 0.1540 - acc: 0.9975 - val_loss: 5.9491 - val_acc: 0.6127\n",
      "Epoch 89/150\n",
      "8053/8053 [==============================] - 29s 4ms/step - loss: 0.1416 - acc: 0.9979 - val_loss: 5.9366 - val_acc: 0.6127\n",
      "Epoch 90/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.1348 - acc: 0.9988 - val_loss: 5.9085 - val_acc: 0.6127\n",
      "Epoch 91/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.1348 - acc: 0.9964 - val_loss: 5.7970 - val_acc: 0.6127\n",
      "Epoch 92/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.1149 - acc: 0.9994 - val_loss: 5.7617 - val_acc: 0.6127\n",
      "Epoch 93/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.1121 - acc: 0.9994 - val_loss: 5.8304 - val_acc: 0.6127\n",
      "Epoch 94/150\n",
      "8053/8053 [==============================] - 26s 3ms/step - loss: 0.1022 - acc: 0.9995 - val_loss: 5.8315 - val_acc: 0.6127\n",
      "Epoch 95/150\n",
      "8053/8053 [==============================] - 25s 3ms/step - loss: 0.0956 - acc: 0.9991 - val_loss: 5.8379 - val_acc: 0.6127\n",
      "Epoch 96/150\n",
      "8053/8053 [==============================] - 25s 3ms/step - loss: 0.0899 - acc: 0.9995 - val_loss: 5.8959 - val_acc: 0.6127\n",
      "Epoch 97/150\n",
      "8053/8053 [==============================] - 27s 3ms/step - loss: 0.0837 - acc: 0.9996 - val_loss: 5.7599 - val_acc: 0.6127\n",
      "Epoch 98/150\n",
      "8053/8053 [==============================] - 26s 3ms/step - loss: 0.0775 - acc: 0.9998 - val_loss: 5.8192 - val_acc: 0.6127\n",
      "Epoch 99/150\n",
      "8053/8053 [==============================] - 25s 3ms/step - loss: 0.0719 - acc: 0.9998 - val_loss: 5.8058 - val_acc: 0.6127\n",
      "Epoch 100/150\n",
      "8053/8053 [==============================] - 25s 3ms/step - loss: 0.0669 - acc: 1.0000 - val_loss: 5.7346 - val_acc: 0.6127\n",
      "Epoch 101/150\n",
      "8053/8053 [==============================] - 26s 3ms/step - loss: 0.0617 - acc: 0.9998 - val_loss: 5.8022 - val_acc: 0.6127\n",
      "Epoch 102/150\n",
      "8053/8053 [==============================] - 25s 3ms/step - loss: 0.0593 - acc: 0.9990 - val_loss: 5.7698 - val_acc: 0.6127\n",
      "Epoch 103/150\n",
      "8053/8053 [==============================] - 27s 3ms/step - loss: 0.0524 - acc: 1.0000 - val_loss: 5.5593 - val_acc: 0.6122\n",
      "Epoch 104/150\n",
      "8053/8053 [==============================] - 25s 3ms/step - loss: 0.0486 - acc: 1.0000 - val_loss: 5.7921 - val_acc: 0.6127\n",
      "Epoch 105/150\n",
      "8053/8053 [==============================] - 25s 3ms/step - loss: 0.0445 - acc: 0.9999 - val_loss: 5.7713 - val_acc: 0.6127\n",
      "Epoch 106/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.0416 - acc: 1.0000 - val_loss: 5.7183 - val_acc: 0.6127\n",
      "Epoch 107/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.0382 - acc: 0.9995 - val_loss: 5.6768 - val_acc: 0.6127\n",
      "Epoch 108/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.0346 - acc: 1.0000 - val_loss: 5.7009 - val_acc: 0.6127\n",
      "Epoch 109/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.0313 - acc: 1.0000 - val_loss: 5.6941 - val_acc: 0.6127\n",
      "Epoch 110/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.0290 - acc: 1.0000 - val_loss: 5.6428 - val_acc: 0.6127\n",
      "Epoch 111/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.0283 - acc: 0.9996 - val_loss: 5.6813 - val_acc: 0.6127\n",
      "Epoch 112/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.0246 - acc: 1.0000 - val_loss: 5.6944 - val_acc: 0.6127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.0237 - acc: 0.9998 - val_loss: 5.6914 - val_acc: 0.6127\n",
      "Epoch 114/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.0205 - acc: 1.0000 - val_loss: 5.6900 - val_acc: 0.6127\n",
      "Epoch 115/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.0199 - acc: 0.9999 - val_loss: 5.6330 - val_acc: 0.6127\n",
      "Epoch 116/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.0181 - acc: 1.0000 - val_loss: 5.6261 - val_acc: 0.6127\n",
      "Epoch 117/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.0160 - acc: 1.0000 - val_loss: 5.6677 - val_acc: 0.6127\n",
      "Epoch 118/150\n",
      "8053/8053 [==============================] - 23s 3ms/step - loss: 0.0150 - acc: 0.9999 - val_loss: 5.6220 - val_acc: 0.6127\n",
      "Epoch 119/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.0134 - acc: 1.0000 - val_loss: 5.6650 - val_acc: 0.6127\n",
      "Epoch 120/150\n",
      "8053/8053 [==============================] - 24s 3ms/step - loss: 0.0129 - acc: 0.9999 - val_loss: 5.6828 - val_acc: 0.6127\n",
      "Epoch 121/150\n",
      "5504/8053 [===================>..........] - ETA: 6s - loss: 0.0120 - acc: 0.9995"
     ]
    }
   ],
   "source": [
    "#full_orig = ['full_paths_original_4', 'full_paths_original_8', 'full_paths_original_16']\n",
    "#full_unres = ['full_paths_unrestricted_4', 'full_paths_unrestricted_8','full_paths_unrestricted_16']\n",
    "#middle_orig = ['middle_paths_original_4', 'middle_paths_original_8', 'middle_paths_original_16']\n",
    "#middle_unres = ['middle_paths_unrestricted_4', 'middle_paths_unrestricted_8', 'middle_paths_unrestricted_16']\n",
    "#'full_paths_original_4',\n",
    "for x in ['middle_paths_unrestricted_16']:\n",
    "\n",
    "    pf = '../code/data/pre_path_{}.csv'.format(x)\n",
    "    name = x\n",
    "    print('=========== {}'.format(name))\n",
    "    lstm = 200\n",
    "    t_model = full_run([\n",
    "            Embedding(vocab_size,100,input_length=maxlen, input_shape=(maxlen, )),\n",
    "            LSTM(lstm,return_sequences=True),\n",
    "            GlobalMaxPooling1D(name='embedding'),\n",
    "    ], pf,\n",
    "        epochs=150,optimizer='rmsprop',batch_size=128,name=name)\n",
    "    t_model.save('{}.h5',name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_frame, data_frame = load_data('../code/data/do-not-touch/heldout_pre_path_middle_paths_unrestricted_16.csv',\n",
    "                                   '../code/data/do-not-touch/held-out-data.csv', remove_no_path=False)\n",
    "    \n",
    "    \n",
    "Xp, yp = preprocess(path_frame,reshaped=False)\n",
    "\n",
    "p = t_model.predict_classes(Xp)\n",
    "from pprint import pprint\n",
    "pprint(yp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
