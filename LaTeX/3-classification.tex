\chapter{Classification of Comparative Sentences}
The data collected from the crowdsourcing task was used as training data for two classification problems. In the first problem, a machine learning algorithm was trained to predict one of the three classes per sentence (see table \ref{tbl:mainstudy-classes}). The second problem is a simplification of the first one as it is designed as a binary classification problem. The classes \texttt{BETTER} and \texttt{WORSE} were merged into the class \texttt{ARG}.

The data was split into a training set (5820 sentences; 4240 \texttt{NONE}, 1102 \texttt{BETTER} and 478 \texttt{WORSE}) and a held-out set.
The experiments were conducted on the training set only. During the development, the experiments were evaluated using stratified k-fold cross-validation where k equals five. 

The held-out set stayed untouched until the final evaluation presented in section \ref{sec:final}.

If not stated otherwise, scikit-learn (\cite{scikit-learn}) was used to perform feature processing, the classification and evaluation.

\section{Choice of Algorithms}
\begin{table}[!h]
\centering

\caption{Tested classification algorithms. Precision, recall and f1 score are calculated as weighted macro-average across classes (as in scikit-learn's \texttt{classification\_report}). The best out of five folds with standard derivation in brackets is presented.}
\label{tbl:algo}
\begin{tabularx}{\textwidth}{Xrrr}
\toprule
Algorithm & Precison & Recall & F1 Score \\ \midrule
\textbf{XGBClassifier} 	&	 \textbf{0.79} \scriptsize{(0.01)} &  \textbf{0.81} \scriptsize{(0.01)} &  \textbf{0.78} \scriptsize{(0.01)}  \\

Logistic Regression 	&	 0.77 \scriptsize{(0.01)} &	 0.79 \scriptsize{(0.01)} &	 0.78 \scriptsize{(0.01)}  \\ 


Stochastic Gradient Descent 	&	 0.76 \scriptsize{(0.01)} &	 0.77 \scriptsize{(0.01)} &	 0.77 \scriptsize{(0.01)}  \\ 

Ada Boost (Decision Trees) 	&	 0.75 \scriptsize{(0.01)} &	 0.78 \scriptsize{(0.00)} &	 0.75 \scriptsize{(0.01)}  \\ 

Support Vector Machine (Linear Kernel) &	 0.76 \scriptsize{(0.01)} &	 0.75 \scriptsize{(0.01)} &	 0.75 \scriptsize{(0.01)}  \\ 


K Neighbors (k = 5) &	 0.77 \scriptsize{(0.03)} &	 0.78 \scriptsize{(0.01)} &	 0.75 \scriptsize{(0.01)}  \\ 


Random Forest &	 0.76 \scriptsize{(0.01)} &	 0.78 \scriptsize{(0.00)} &	 0.75 \scriptsize{(0.01)}  \\ 

Decision Tree &	 0.74 \scriptsize{(0.01)} &	 0.75 \scriptsize{(0.01)} &	 0.74 \scriptsize{(0.01)}  \\ 

Extra Trees 	&	 0.74 \scriptsize{(0.01)} &	 0.78 \scriptsize{(0.00)} &	 0.74 \scriptsize{(0.00)}  \\ 

Multinomial Gaussian Bayes &	 0.69 \scriptsize{(0.04)} &	 0.77 \scriptsize{(0.01)} &	 0.71 \scriptsize{(0.01)}  \\ 


Support Vector Machine (RBF Kernel) 	&	 0.53 \scriptsize{(0.00)} &	 0.73 \scriptsize{(0.00)} &	 0.61 \scriptsize{(0.00)}  \\ 

Support Vector Machine (Polynomial Kernel) 	&	 0.53 \scriptsize{(0.00)} &	 0.73 \scriptsize{(0.00)} &	 0.61 \scriptsize{(0.00)}  \\

Support Vector Machine (Sigmoid Kernel) &	 0.53 \scriptsize{(0.00)} &	 0.73 \scriptsize{(0.00)} &	 0.61 \scriptsize{(0.00)}  \\ 


\bottomrule
\end{tabularx}
\end{table}

To find the best performing classification algorithms, thirten (see table \ref{tbl:algo}) were selected and compared. Except \emph{XGBoost}\footnote{XGBoost is not part of scikit-learn. The implementation presented in \cite{DBLP:journals/corr/ChenG16} was used.} and \emph{Extra Trees Classifier}, all algorithms were used in at least one paper presented in section \ref{sec:argmine}. The binary unigram vector computed on the whole sentence was used as the only feature. Stratified k-fold with k equals five was used to assess the quality of the algorithm.





%\begin{table}[h]
%\centering
%\label{tbl:algo}
%\caption{All evaluated classification algorithms. The F1 score shows the classification performance with the unigram feature.}
%\begin{tabularx}{\textwidth}{XlX}
%\toprule
%Algorithm & F1 Score & Used in\\ \midrule
%
%XGBoost & 0.76 & -\\ 
%
%Logistic Regression & 0.75 &  \cite{Dusmanu2017Argument-Mining,Daxenberger2017What-is-the-EssAker2017What-works-and-,Lippi2016Argumentation-M}\\ 
%
%AdaBoost & 0.74 &  \cite{Aker2017What-works-and-}\\  
%
%Linear SVC & 0.73 & \cite{Aker2017What-works-and-}\\  
%
%Decision Tree & 0.72 &  \cite{Stab2014Identifying-Arg,Lippi2016Argumentation-M}\\ 
%
%Stochastic Gradient Descent & 0.72 & -\\ 
%
%
%Random Forest & 0.72 &  \cite{Dusmanu2017Argument-Mining,Stab2014Identifying-Arg,Eckle-Kohler2015On-the-Role-of-,Aker2017What-works-and-,Lippi2016Argumentation-M}\\  
%
%Extra Trees Classifier & 0.72 & - \\
%
%K Neighbors & 0.72 &  \cite{Aker2017What-works-and-}\\ 
%
%
%Support Vector Machine (non-linear kernel) & 0.60 & \cite{Stab2014Identifying-Arg,Eckle-Kohler2015On-the-Role-of-,Park:2012:ICC:2391171.2391173,Lippi2016Argumentation-M,Habernal2016Argumentation-M}\\ 
%
%
%Naive Bayes & 0.60 & \cite{Stab2014Identifying-Arg,Eckle-Kohler2015On-the-Role-of-,Aker2017What-works-and-,Park:2012:ICC:2391171.2391173,Lippi2016Argumentation-M}\\  
%
%
% \bottomrule 
%\end{tabularx}
%
%\end{table}


Tree-based methods and linear models work good. Support Vector Machines with non-linear kernels assigne \texttt{NONE} to all sentences.

As XGBoost and Logistic Regression already work in a pleasing way, no further investigations on the performance of other algorithms were done. A set of hyper-parameters for XGBoost were tested using exhaustive grid search and randomized search. However, no significant increase in the F1 score could be achieved.

In the following sections, all experiments were done using XGBoost.


\section{Preprocessing}
In addition to the full sentence, preprocessed versions of it were used in the feature calculation.

Different parts of the sentence were used. The \emph{first part} contains all words from the beginning of the sentence to the first object, while the \emph{last part} contains all words from the second object to the end of the sentence. The \emph{middle part} contains all words between the first and the second object.

Another processing step was done to assess how important the objects are for the classification. The objects either stayed untouched, were removed or replaced. Two different replacement strategies were tested. First, both objects were replaced by the term \emph{OBJECT} (replacement). Second, the first object was replaced by \emph{OBJECT\_A} and the second by \emph{OBJECT\_B} (distinct replacement). Some examples are shown in table \ref{preprocessing_example}.

\begin{table}[h]
\centering

\caption{Preprocessing examples for the sentence \enquote{\emph{In my mind, Python is better than Ruby}}}
\label{preprocessing_example}
\begin{tabularx}{\linewidth}{lX}
\toprule
Preprocessing & Result \\ \midrule
Middle part & Python is better than Ruby \\
Middle part, removal & is better than \\
Full sentence, distinct replacement &In my mind, OBJECT\_A is better than OBJECT\_B \\
First part, removal & In my mind, \\
\bottomrule
\end{tabularx}

\end{table}


\section{Features}


\section{Classification with three classes}
\subsection{Baseline}
\label{sec:3_baseline}
As described in section \ref{sec:argmine}, there is no task which is similar enough to this one which could be used as a baseline. Thus, two baselines using the obtained data were created. The first baseline, shown in table \ref{tbl:3stratifiedbaseline}, assigns all sentences to the class \texttt{NONE}.

% 24.3
\begin{table}[!htb]
    \begin{minipage}{.5\linewidth}
      \caption{Random (stratified) baseline for the classification task.}
      \label{tbl:3stratifiedbaseline}
      \centering
      
\begin{tabular}{@{}lrrrr@{}}
\toprule
 	&	 precision &	 recall &	 f1 score  \\ \midrule 
\texttt{BETTER}	&	 0.22 \scriptsize{(0.02)} &	 0.23 \scriptsize{(0.02)} &	 0.22 \scriptsize{(0.02)}  \\ 
\texttt{WORSE}	&	 0.10 \scriptsize{(0.02)} &	 0.08 \scriptsize{(0.02)} &	 0.09 \scriptsize{(0.02)}  \\ 
\texttt{NONE}	&	 0.74 \scriptsize{(0.01)} &	 0.74 \scriptsize{(0.01)} &	 0.74 \scriptsize{(0.01)}  \\ 
average	&	 0.59 \scriptsize{(0.01)} &	 0.59 \scriptsize{(0.01)} &	 0.59 \scriptsize{(0.01)}  \\ 
\bottomrule
\end{tabular} 

  \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering
        \caption{Majority class baseline for the classification task.}
        \label{tbl:3majoritybaseline}
\begin{tabular}{@{}lrrrr@{}}
\toprule
 	&	 precision &	 recall &	 f1 score  \\ \midrule 
\texttt{BETTER}	&	 0.00 \scriptsize{(0.00)} &	 0.00 \scriptsize{(0.00)} &	 0.00 \scriptsize{(0.00)}  \\ 
\texttt{WORSE}	&	 0.00 \scriptsize{(0.00)} &	 0.00 \scriptsize{(0.00)} &	 0.00 \scriptsize{(0.00)}  \\ 
\texttt{NONE}	&	 0.73 \scriptsize{(0.00)} &	 1.00 \scriptsize{(0.00)} &	 0.84 \scriptsize{(0.00)}  \\ 
average	&	 0.53 \scriptsize{(0.00)} &	 0.73 \scriptsize{(0.00)} &	 \textbf{0.61} \scriptsize{(0.00)}  \\ 
\bottomrule
\end{tabular}
    \end{minipage} 
\end{table}



The second baseline is created by assigning classes to the data at random, respecting the distribution of classes in the original data. The results are shown in table \ref{tbl:3majoritybaseline}.

For both baselines, the scikit-learn's \texttt{DummyClassifer} was used.



\subsection{Results}
The classification results are presented in the tables below. Every table shows the best out of five folds. The standard derivation is stated in brackets. Top results are printed in bold. Only the best feature configurations are shown.

Tables \ref{tbl:se3} to \ref{tbl:ngram_3} show the results of different vector representations of the sentence. Simple unigrams and the more complex sentence embeddings yield almost the same performance. The main difference is with the class \texttt{WORSE}.


\begin{table}[h]
    \begin{minipage}{.5\linewidth}
    
        \caption{\emph{Sentence embeddings on the middle part of the sentence}. (Standard derivation)} 
        \label{tbl:se3}
        \begin{tabular}{@{}lrrrr@{}}
\toprule
 	&	 precision &	 recall &	 f1 score  \\ \midrule 
\texttt{BETTER}	&	 0.81 \scriptsize{(0.04)} &	 0.79 \scriptsize{(0.02)} &	 0.80 \scriptsize{(0.03)}  \\ 
\texttt{WORSE}	&	 0.57 \scriptsize{(0.04)} &	 \textbf{0.33} \scriptsize{(0.05)} &	 \textbf{0.42} \scriptsize{(0.04)}  \\ 
\texttt{NONE}	&	 \textbf{0.91} \scriptsize{(0.01)} &	 0.96 \scriptsize{(0.01)} &	 0.93 \scriptsize{(0.01)}  \\ 
average	&	 0.86 \scriptsize{(0.01)} &	 0.87 \scriptsize{(0.01)} &	 \textbf{0.86} \scriptsize{(0.01)}  \\ 
\bottomrule
        \end{tabular} 
  \end{minipage} \hfill
    \begin{minipage}{.5\linewidth}
    \caption{ \emph{Binary unigram vector} for all unigrams in the middle part of the sentence. } 
          \begin{tabular}{@{}lrrrr@{}}
\toprule
 	&	 precision &	 recall &	 f1 score  \\ \midrule 
\texttt{BETTER}	&	 \textbf{0.82} \scriptsize{(0.03)} &	 0.79 \scriptsize{(0.04)} &	 \textbf{0.81} \scriptsize{(0.03)}  \\ 
\texttt{WORSE}	&	 \textbf{0.71} \scriptsize{(0.06)} &	 0.23 \scriptsize{(0.02)} &	 0.35 \scriptsize{(0.02)}  \\ 
\texttt{NONE}	&	 0.89 \scriptsize{(0.01)} &	 \textbf{0.97} \scriptsize{(0.01)} &	 0.93 \scriptsize{(0.01)}  \\ 
average	&	 0.86 \scriptsize{(0.01)} &	 0.87 \scriptsize{(0.01)} &	 \textbf{0.86} \scriptsize{(0.01)}  \\ 
\bottomrule
            \end{tabular}
    \end{minipage} 
\end{table}

Mean Word Embeddings (table \ref{tbl:3_mwe}) and POS n-grams (table \ref{tbl:ngram_3}) are barely able to recognize \texttt{WORSE} at all.

\begin{table}[h]
    \begin{minipage}{.5\linewidth}
   \caption{ \emph{Mean Word Embeddings} for the middle part of the sentence. } 
    \label{tbl:3_mwe}
\begin{tabular}{@{}lrrrr@{}}
\toprule
 	&	 precision &	 recall &	 f1 score  \\ \midrule 
\texttt{BETTER}	&	 0.70 \scriptsize{(0.01)} &	 0.75 \scriptsize{(0.04)} &	 0.72 \scriptsize{(0.02)}  \\ 
\texttt{WORSE}	&	 0.44 \scriptsize{(0.16)} &	 0.04 \scriptsize{(0.04)} &	 0.08 \scriptsize{(0.07)}  \\ 
\texttt{NONE}	&	 0.88 \scriptsize{(0.01)} &	 0.96 \scriptsize{(0.01)} &	 0.92 \scriptsize{(0.00)}  \\ 
average	&	 0.81 \scriptsize{(0.01)} &	 0.84 \scriptsize{(0.01)} &	 0.81 \scriptsize{(0.01)}  \\ 
\bottomrule
\end{tabular}
  \end{minipage} \hfill
    \begin{minipage}{.5\linewidth}
  
     \caption{500 most frequent \emph{part-of-speech bi-, tri- and four-grams}; weighted by tf-idf.} 
       \label{tbl:ngram_3}
\begin{tabular}{@{}lrrrr@{}}
\toprule
 	&	 precision &	 recall &	 f1 score  \\ \midrule 
\texttt{BETTER}	&	 0.63 \scriptsize{(0.04)} &	 0.67 \scriptsize{(0.05)} &	 0.65 \scriptsize{(0.04)}  \\ 
\texttt{WORSE}	&	 0.64 \scriptsize{(0.19)} &	 0.07 \scriptsize{(0.02)} &	 0.13 \scriptsize{(0.04)}  \\ 
\texttt{NONE}	&	 0.87 \scriptsize{(0.01)} &	 0.94 \scriptsize{(0.01)} &	 0.90 \scriptsize{(0.01)}  \\ 
average	&	 0.80 \scriptsize{(0.03)} &	 0.82 \scriptsize{(0.01)} &	 0.79 \scriptsize{(0.01)}  \\ 
\bottomrule
\end{tabular}
    \end{minipage} 
\end{table}


Surprisingly, a single boolean feature (table \ref{tbl:3jjr}) can yield an f1 score fiften points higher than the majority class baseline. However, it does not recognize any sentences of the class \texttt{WORSE}.

\begin{table}[h] 
 \centering 
 \caption{Boolean feature, capturing the appearance of the part-of-speech JJR in the middle part of the sentence.} 
 \label{tbl:3jjr}
\begin{tabular}{@{}lrrrr@{}}
\toprule
 	&	 precision &	 recall &	 f1 score  \\ \midrule 
\texttt{BETTER}	&	 0.60 \scriptsize{(0.02)} &	 0.58 \scriptsize{(0.02)} &	 0.59 \scriptsize{(0.02)}  \\ 
\texttt{WORSE}	&	 0.00 \scriptsize{(0.00)} &	 0.00 \scriptsize{(0.00)} &	 0.00 \scriptsize{(0.00)}  \\ 
\texttt{NONE}	&	 0.84 \scriptsize{(0.01)} &	 0.94 \scriptsize{(0.01)} &	 0.89 \scriptsize{(0.01)}  \\ 
average	&	 0.73 \scriptsize{(0.01)} &	 0.80 \scriptsize{(0.01)} &	 0.76 \scriptsize{(0.01)}  \\ 
\bottomrule
\end{tabular}
\end{table}


Using only the middle part of the sentence increased the f1 score by five to seven points every time. The removal or replacement of the object only altered the f1 score by about 0.01 points, which is equal to the standard derivation.



\subsection{Error analyis}


\section{Binary classification}
\subsection{Baseline}
% 24.3
The baselines for the binary classification task are shown in tables \ref{tbl:binmaj} and \ref{tbl:binstrat}.


\begin{table}[!htb]
    \begin{minipage}{.5\linewidth}
      \caption{Random (stratified) baseline for the binary classification task.}
      \label{tbl:binmaj}
      \centering
      
\begin{tabular}{@{}lrrrr@{}}
\toprule
 	&	 precision &	 recall &	 f1 score  \\ \midrule 
\texttt{ARG}	&	 0.31 \scriptsize{(0.02)} &	 0.31 \scriptsize{(0.02)} &	 0.31 \scriptsize{(0.02)}  \\ 
\texttt{NONE}	&	 0.74 \scriptsize{(0.01)} &	 0.74 \scriptsize{(0.01)} &	 0.74 \scriptsize{(0.01)}  \\ 
average	&	 0.62 \scriptsize{(0.01)} &	 0.62 \scriptsize{(0.01)} &	 \textbf{0.62} \scriptsize{(0.01)}  \\ 
\bottomrule
\end{tabular}

  \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering
        \caption{Majority class baseline for the binary classification task.}
        \label{tbl:binstrat}
\begin{tabular}{@{}lrrrr@{}}
\toprule
 	&	 precision &	 recall &	 f1 score  \\ \midrule 
\texttt{ARG}	&	 0.00 \scriptsize{(0.00)} &	 0.00 \scriptsize{(0.00)} &	 0.00 \scriptsize{(0.00)}  \\ 
\texttt{NONE}	&	 0.73 \scriptsize{(0.00)} &	 1.00 \scriptsize{(0.00)} &	 0.84 \scriptsize{(0.00)}  \\ 
average	&	 0.53 \scriptsize{(0.00)} &	 0.73 \scriptsize{(0.00)} &	 0.61 \scriptsize{(0.00)}  \\ 
\bottomrule
\end{tabular}
    \end{minipage} 
\end{table}
They were generated the same way as in section \ref{sec:3_baseline}.





\subsection{Results}
\subsection{Error analyis}
\section{Discussion}
The results show that the part between the two objects of interest is most valuable. Also, the objects are not important at all for the classification. Removing or replacing the objects from the sentence did not decrease any score significantally.

Simple features, like the bag-of-words model yielding results similar to the more complexe sentence embeddings.

\section{Evaluation with the held-out data}
\label{sec:final}

