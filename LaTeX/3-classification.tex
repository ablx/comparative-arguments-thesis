\chapter{Classification of Comparative Sentences}
\section{Experiments}
The data collected from the crowdsourcing task was used as training data for classification problems. In the first problem, the machine learning algorithms were trained so that they were able to assign one of the three original classes (see section \ref{sec:designchanges}) to the data. The second problem is a simplification of the first one as it is designed as a binary classification problem. The classes \texttt{BETTER} and \texttt{WORSE} were merged into the class \texttt{ARG}.


The data was split into a training set (5937 examples; 4270 \texttt{NONE}, 1158 \texttt{BETTER} and 509 \texttt{WORSE}) and a held-out set. The held-out set stayed untouched until the final evaluation presented in section \ref{sec:final}. During the development, the experiments were evaluated using stratified k-fold cross-validation where k equals five. The evaluation was done with the scikit-learn implementation. As the evalutation metric, the weighted mean\footnote{weighted by number of examples per class} of the F1 scores of all classes is used. 

\section{Choice of Algorithms}

To find the best performing classification algorithms, eleven (see table \ref{tbl:algo}) were selected and compared. For all algorithms under test, the unigrams of the whole sentence were used as binary features (the implementation is described in section \ref{sec:ngrams}). Unigrams are a simple yet efficient feature for text classification (\cite{cavnar1994n}) which makes them suitable as a baseline feature for comparisons. Stratified k-fold with k equals five was used to assess the quality of the algorithm. The unweighted average of the five F1 scores was used to make the different algorithms more comparable.

\begin{table}[h]
\centering
\label{tbl:algo}
\caption{All evaluated classification algorithms. The F1 score shows the classification performance with the unigram feature.}
\begin{tabularx}{\textwidth}{XlX}
\toprule
Algorithm & F1 Score & Used in\\ \midrule

XGBoost & 0.76 & -\\ 

Logistic Regression & 0.75 &  \cite{Dusmanu2017Argument-Mining,Daxenberger2017What-is-the-EssAker2017What-works-and-,Lippi2016Argumentation-M}\\ 

AdaBoost & 0.74 &  \cite{Aker2017What-works-and-}\\  

Linear SVC & 0.73 & \cite{Aker2017What-works-and-}\\  

Decision Tree & 0.72 &  \cite{Stab2014Identifying-Arg,Lippi2016Argumentation-M}\\ 

Stochastic Gradient Descent & 0.72 & -\\ 


Random Forest & 0.72 &  \cite{Dusmanu2017Argument-Mining,Stab2014Identifying-Arg,Eckle-Kohler2015On-the-Role-of-,Aker2017What-works-and-,Lippi2016Argumentation-M}\\  

Extra Trees Classifier & 0.72 & - \\

K Neighbors & 0.72 &  \cite{Aker2017What-works-and-}\\ 


Support Vector Machine (non-linear kernel) & 0.60 & \cite{Stab2014Identifying-Arg,Eckle-Kohler2015On-the-Role-of-,Park:2012:ICC:2391171.2391173,Lippi2016Argumentation-M,Habernal2016Argumentation-M}\\ 


Naive Bayes & 0.60 & \cite{Stab2014Identifying-Arg,Eckle-Kohler2015On-the-Role-of-,Aker2017What-works-and-,Park:2012:ICC:2391171.2391173,Lippi2016Argumentation-M}\\  


 \bottomrule 
\end{tabularx}

\end{table}


For all algorithms except XGBoost, the implementation available from scikit-learn (\cite{scikit-learn} ) was used. Tree-based methods and simple linear models work good. The \emph{Support Vector Machine} without a linear kernel was not able to classify the examples at all as it assigns \texttt{NONE} to all training examples. This is true for all non-linear kernel functions available in scikit-learn (RBF, polynomial and Sigmoid).

Naive Bayes classifies examples correctly into all classes, yet the performance is still same as the baseline shown in section \ref{sec:baseline}.

As XGBoost and Logistic Regression already work in a pleasing way, no further investigations on the performance of the \emph{Support Vector Machine} and \emph{Naive Bayes} were done. In the following sections, all experiments were done using XGBoost.

A set of hyper-parameters for XGBoost were tested using exhaustive grid search and randomized search. However, no significant increase in the F1 score could be achieved.








\section{Features}
The following section briefly describes each feature used for both classification tasks. 

Two observations are true for all features. First, the F1 score increases by at least five points if only the part of the sentence between both objects of interest is used (from now on called \enquote{\emph{middle part}}).

 Second, the objects do not make any significant difference. All features were tested with four versions of the sentence (eventually with the middle part only). 
\begin{enumerate}
\item the sentence stayed unchanged
\item the objects of interest removed completely from the sentence
\item both objects replaced with \emph{OBJECT}
\item the first object replaced with \emph{OBJECT\_A}, the second object replaced with \emph{OBJECT\_B}
\end{enumerate}
Most of the time, the F1 score for the different versions differ - if at all - in the third decimal place. In exceptional cases, using the fourth option increases the F1 score by X points. If not stated differently, all features used the unchanged middle part of the original sentence.


\subsection{Bag-Of-Words Feature}
Some variants of the BOW model present in section \ref{sec:bow_model} have been used. Binary feature vectors\footnote{the n-grams were computed using \emph{textacy} (https://github.com/chartbeat-labs/textacy)} and their tf-idf weighted\footnote{relying on the scikit-learn implementation} counterparts were tested for uni-, bi- and trigrams. No restrictions were imposed on the n-grams.

Uni- and bigram models bet both baselines.

The idea behind using trigram models despite the short text length was to capture trigrams like \enquote{\emph{is better than}} or \enquote{\emph{is worse because}} which would indicate a comparison. However, the performance of trigram models was not satisfying.
\label{sec:ngrams}

\subsection{Part of Speech}
Several boolean features were used to model the apperance of parts-of-speech in the sentence. The part-of-speech tagging was done with \emph{spaCy}. The feature capturing the apperance of the part-of-speeches \emph{comparative adverb (RBR)} and \emph{comparative adjective (JJR)}\footnote{https://www.ling.upenn.edu/courses/Fall\_2003/ling001/penn\_treebank\_pos.html} had an equal performance to the unigram model. Other part-of-speech tags did not get a result above the baseline (for instance \emph{superlative adjective (RBS)} However, using this feature in isolation, no training example is assigned to the class ?

\subsection{Mean Word Embeddings}

\subsection{Sentence Embeddings}
\subsection{Dependency Features}
\subsection{Other Features}
A couple of features did not contribute to the classification at all, be it alone or in combination with other features. This includes the length of the sentence or its parts, statistics over punctuation or named entities.
% n-grams
% sentence embeddings
% POS JJR

\section{Classification with three classes}
\subsection{Baseline}
\label{sec:baseline}
As described in section \ref{sec:argmine}, there is no task which is similar enough to this one which could be used as a baseline. Thus, two baselines using the obtained data were created. The first baseline, shown in table \ref{foo}, assigns all sentences to the class \texttt{NONE}.


\begin{table}[!htb]
    \begin{minipage}{.5\linewidth}
      \caption{Random baseline for the binary classification task.}
      \label{foo}
      \centering
\begin{tabular}{@{}lrrrr@{}}
\toprule
 	&	 precision &	 recall &	 f1 score  \\ \midrule 
ARG	&	 0.00 \scriptsize{(0.00)} &	 0.00 \scriptsize{(0.00)} &	 0.00 \scriptsize{(0.00)}  \\ 
NONE	&	 0.80 \scriptsize{(0.03)} &	 1.00 \scriptsize{(0.00)} &	 0.89 \scriptsize{(0.02)}  \\ 
average	&	 0.64 \scriptsize{(0.04)} &	 0.80 \scriptsize{(0.03)} &	 0.71 \scriptsize{(0.04)}  \\ 
\bottomrule
\end{tabular}
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering
        \caption{Majority class baseline for the binar classification task.}
\begin{tabular}{@{}lrrrr@{}}
\toprule
 	&	 precision &	 recall &	 f1 score  \\ \midrule 
ARG	&	 0.00 \scriptsize{(0.00)} &	 0.00 \scriptsize{(0.00)} &	 0.00 \scriptsize{(0.00)}  \\ 
NONE	&	 0.80 \scriptsize{(0.03)} &	 1.00 \scriptsize{(0.00)} &	 0.89 \scriptsize{(0.02)}  \\ 
average	&	 0.64 \scriptsize{(0.04)} &	 0.80 \scriptsize{(0.03)} &	 0.71 \scriptsize{(0.04)}  \\ 
\bottomrule
\end{tabular}
    \end{minipage} 
\end{table}



The second baseline is created by assigning classes to the data at random, respecting the distribution of classes in the original data. The results are shown in table \ref{tbl:a}.

For both baselines, the scikit-learn's \texttt{DummyClassifer} was used.



\subsection{Results}

\begin{table}[!htb]
    \caption{Global caption}
    \begin{minipage}{.52\linewidth}
      \caption{Bla blubb}
      \centering
 \begin{tabular}{@{}lrrrr@{}}
\toprule
 	&	 precision &	 recall &	 f1 score  \\ \midrule 
\texttt{BETTER}	&	 0.63 \scriptsize{(0.06)} &	 0.34 \scriptsize{(0.03)} & 0.45 \scriptsize{(0.03)}  \\ 
\texttt{WORSE}	&	 0.33 \scriptsize{(0.16)} &	 0.01 \scriptsize{(0.01)} & 0.02 \scriptsize{(0.01)}  \\ 
\texttt{NONE}	&	 0.79 \scriptsize{(0.01)} &	 0.97 \scriptsize{(0.01)} & 0.87 \scriptsize{(0.00)}  \\ \midrule
average	&	 0.72 \scriptsize{(0.02)} &	 0.77 \scriptsize{(0.01)} & 0.72 \scriptsize{(0.01)}  \\ 
\bottomrule
\end{tabular}
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering
        \caption{Blubb}
 \begin{tabular}{@{}lrrrr@{}}
\toprule
 	&	 precision &	 recall &	 f1 score  \\ \midrule 
\texttt{BETTER}	&	 0.63 \scriptsize{(0.06)} &	 0.34 \scriptsize{(0.03)} & 0.45 \scriptsize{(0.03)}  \\ 
\texttt{WORSE}	&	 0.33 \scriptsize{(0.16)} &	 0.01 \scriptsize{(0.01)} & 0.02 \scriptsize{(0.01)}  \\ 
\texttt{NONE}	&	 0.79 \scriptsize{(0.01)} &	 0.97 \scriptsize{(0.01)} & 0.87 \scriptsize{(0.00)}  \\ \midrule
average	&	 0.72 \scriptsize{(0.02)} &	 0.77 \scriptsize{(0.01)} & 0.72 \scriptsize{(0.01)}  \\ 
\bottomrule
\end{tabular}
    \end{minipage} 
\end{table}

%==== result table     


\section{Binary classification}
\subsection{Baseline}

Table \ref{tbl:bin_maj}; Table \ref{tbl:bin_strat}

 \begin{table}[h]
                \centering
\caption{Random baseline for the binary classification task. The classes are assigned at random, but the distribution in the original data set is kept.}
\label{tbl:bin_strat}
 \begin{tabular}{@{}lccccccccc@{}}
              \toprule
               & \multicolumn{3}{c}{Worst} & \multicolumn{3}{c}{Average} & \multicolumn{3}{c}{Best}  \\ \midrule
               & Precision  & Recall & F1   & Precision  & Recall  & F1    & Precision & Recall & F1   \\ \toprule
\texttt{ARG}	 & 0.25	 & 0.26	 & 0.25	 &0.29	 & 0.29	 & 0.29	 &0.30	 & 0.30	 & 0.30	 \\ 
\texttt{NONE}	 & 0.71	 & 0.70	 & 0.71	 &0.72	 & 0.72	 & 0.72	 &0.73	 & 0.72	 & 0.72	 \\ \midrule 
average	 & 0.58	 & 0.58	 & 0.58	 &0.60	 & 0.60	 & 0.60	 &0.61	 & 0.60	 & 0.60	 \\ \bottomrule

    \end{tabular}
\end{table}

 \begin{table}[h]
                \centering
\caption{Majority class baseline for the binar classification task.}
\label{tbl:bin_maj}
 \begin{tabular}{@{}lccccccccc@{}}
              \toprule
               & \multicolumn{3}{c}{Worst} & \multicolumn{3}{c}{Average} & \multicolumn{3}{c}{Best}  \\ \midrule
               & Precision  & Recall & F1   & Precision  & Recall  & F1    & Precision & Recall & F1   \\ \toprule
\texttt{ARG}	 & 0.00	 & 0.00	 & 0.00	 &0.00	 & 0.00	 & 0.00	 &0.00	 & 0.00	 & 0.00	 \\ 
\texttt{NONE}	 & 0.72	 & 1.00	 & 0.84	 &0.72	 & 1.00	 & 0.84	 &0.72	 & 1.00	 & 0.84	 \\ \midrule 
average	 & 0.52	 & 0.72	 & 0.60	 &0.52	 & 0.72	 & 0.60	 &0.52	 & 0.72	 & 0.60	 \\ \bottomrule

    \end{tabular}
\end{table}

\subsection{Results}

\section{Final results}
\label{sec:final}

\section{Discussion}