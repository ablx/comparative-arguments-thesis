\chapter{Building a data set for Comparative Argument Mining}
Due to the novelty of Argument Mining (and especially Comparative Argument Mining), the supply of datasets is small. Thus, a new data set had to be created.

This dataset was designed to answer the questions if a given sentence compares two known objects, and if it does, if the first-mentioned object is better or worse than the second one. Those questions will be translated to several classification tasks in the later chapters.

The dataset was created using the crowdsourcing platform CrowdFlower\footnote{https://www.crowdflower.com 23.02.2018}. As described in detail in the following chapters, the annotators were asked to assign one of four (and later three) classes to a sentence in which the objects of interest are highlighted.

The final dataset consists of 7421 sentences, each containing one of 273 object pairs. The sentences were labelled with one of three classes. Each sentence was at least annotated by three different annotators.

\section{Common Crawl Text Corpus}
The sentences for the crowdsourcing campaign were obtained from a CommonCrawl\footnote{https://commoncrawl.org 23.02.2018} dataset. CommonCrawl is a non-profit organisation which crawls the web and releases the crawled data for free use.

The data used in this thesis was already preprocessed\footnote{Download Link} (see \cite{Panchenko:2017aa}). First, it contains only English text. Duplicates and near-duplicates were removed, as well as all HTML tags. The texts were then split into sentences.

The resulting sentences were used to obtain the sentences for the crowdsourcing campaign. To make them manageable, an ElasticSearch index (from now on called "the index") was created. The index contains 3,288,963,864 unique sentences.

To get an idea if there are enough comparative sentences in the index, it was queried for all sentences containing one of the words \enquote{\emph{better}} or \enquote{\emph{worse}},  as those words often indicate a comparison. This query returns 32,946,247 matching sentences. Querying for \enquote{\emph{is better than}} still returns 428,932 sentences.

Those numbers show that there are enough sentences in the index to create a dataset for the given task. Even if only 1\% of the sentences containing \enquote{\emph{is better than}} are truly comparative, there would be 4289 training examples for the machine learning algorithm.


\section{Prestudies}
Before the mainstudy could start, several questions had to be answered.

First, how to extract sentences from the index? Second, how to preprocess those sentences? Third, which labels should be assigned to the sentences? Fourth, how to phrase the guidelines?

Two prestudies were conducted to answer those questions.



\subsection{Sentence Selection}
The sentences for the crowdsourcing campaign should have a high probability of being comparative so that enough positive examples for the machine learning part are present. To ensure this, a list of cue words which indicate comparison was compiled. For the prestudy, those words were \enquote{\emph{better}}, \enquote{\emph{worse}}, \enquote{\emph{inferior}}, \enquote{\emph{superior}}, and \enquote{\emph{because}}. Comparable objects are needed as well. A list of object pairs was selected by hand (see table \ref{tbl:prestudy-objects}). The pairs were selected in a way that they span a wide range of different domains, such as programming languages, countries and pets. The idea behind this is that pets are compared differently than programming languages. In this way, there will be different comparison patterns in the data.

\begin{table}[h]
\centering
\caption{Objects of the Annotation Prestudy}
\label{tbl:prestudy-objects}
\begin{tabular}{@{}llrrr@{}}
\toprule
First Object & Second Object      & \# Sentences                             \\ \midrule
Ruby    & Python    & 100      \\
BMW    & Mercedes    & 100  \\
USA & Europe & 100 \\
Beef & Chicken & 100   \\
Android & iPhone    &   100  \\
Cat & Dog      &     100  \\ 
Football & Baseball   &  100 \\ 
Wine & Beer  & 100  \\
Car & Bicycle & 100 \\
Summer & Winter &  100\\
\bottomrule  
                               
\end{tabular}
\end{table}

However, not all comparisons will contain one of the cue words mentioned above. Two different queries were used to overcome the coverage problem. Sevenhundred-fivtey sentences were obtained using query \ref{lst:es-query-a} (seventy-five for each pair) and 250 using query \ref{lst:es-query-b} (twenty-five for each pair). The second query will also match not-anticipated sentences such as \enquote{\emph{I like X more than Y since Z.}}.



\begin{lstlisting}[label=lst:es-query-a,breaklines=true,postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},caption=Prestudy Sentence Selection Query A]
{ "query":{  "bool":{ "must":[ {
          "query_string":{
            "default_field":"text",
            "query":"(better OR worse OR superior OR inferior) AND \"<OBJECT_A>\" AND \"<OBJECT_B>\""
          }
        } ] } } }
\end{lstlisting}

\begin{lstlisting}[label=lst:es-query-b,breaklines=true,postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},caption=Prestudy Sentence Selection Query B (shortened)]
[...]
          "query_string":{
            "default_field":"text",
            "query":" \"<OBJECT_A>\" AND \"<OBJECT_B>\""
[...]
\end{lstlisting}

Table \ref{tbl:example_sentences} shows some sentences obtained with this method. The objects of interest ar printed in italics.

\begin{table}[h]
\centering
\caption{Example Sentences}
\label{tbl:example_sentences}
\begin{tabular}{@{}llr@{}}
\toprule
\# & Sentence   &  Cue Words Used                      \\ \midrule
1 & He's the best pet that you can get, Better than a \emph{dog} or \emph{cat}. & Yes \\
2 &\emph{Android} phones have better processing power than \emph{iPhone} & Yes \\
3 & 10 Things \emph{Android} Does Better Than \emph{iPhone} OS & Yes \\
4 & \emph{Dog} scared of \emph{cat} & No \\
5 & In fact, many 'supercars' will use \emph{BMW} or \emph{Mercedes} engines. & No \\

\bottomrule  
                               
\end{tabular}
\end{table}



\subsection{Prestudy A}

\subsection{Prestudy B}

\subsection{Task}
Using the method described above, 1050 sentences were obtained for the prestudy. The annotators were asked to assigne one of the following classes to the sentences. Each sentence was annotated by three annotators.

The annotators where asked to assign one of the four classes (see table \ref{tbl:prestudyclasses}) to each sentence.

\begin{table}[h]
\centering
\caption{Classes for the Prestudy}
\label{tbl:prestudyclasses}
\begin{tabular}{@{}ll@{}}
\toprule
Class & Description \\ \midrule
BETTER & The first object in the sentence (object A) is better than the second one (object B)\\
WORSE & The first object is worse \\
UNCLEAR & Neither BETTER nor WORSE fits, but the sentence is comparative\\
NO\_COMP & The sentence is not comparative\\
\bottomrule
\end{tabular}
\end{table}



In a first step, 100 sentences were annotated. To ensure the quality, twelve additional sentences were setup as test sentences. If one annotator failed three test sentences, he was removed from the task.

The sentences were preprocessed: the first object was replaced by OBJECT\_A, the second by OBJECT\_B. Examples are shown in table \ref{tbl:pre1s}. The removal was done so that the annotators can concentrate on the comparative structure of the sentence and are not biased by the objects.


% Beispielsätze PreStudy 1. Teil
\begin{table}[h]
\centering
\caption{Sentences for the first step}
\label{tbl:pre1s}
\begin{tabular}{{p{12cm}p{3cm}}}
\toprule
Sentence            & Expected Class \\ \midrule
This is potentially useful for OBJECT\_A, PHP, JS and OBJECT\_B.                                 & NO\_COMP       \\
Also keep in mind that OBJECT\_A blends will give you worse mileage than OBJECT\_B & WORSE      \\ 
Snowboarding during OBJECT\_A is a lot better than during OBJECT\_B. & BETTER \\
\bottomrule
\end{tabular}
\end{table}



This test step delivered valuable insights. First, the amount of test sentences was to small. Users might see the same test sentence twice. Second, the phrasing of the annotation guidelines was to confusing, especially the distinction between NO\_COMP and UNCLEAR as well as their class names.
Third, the complete removal of the original objects is suspected to partly obscure the sense of the sentences.\newline

In a second step, 200 new sentences were annotated, again with three annotations per sentences. This time, 51 test questions were used, so that it is less likely that annotators will see the same question twice. Furthermore, the preprocessing was changed. Instead of removing the original objects, :[OBJECT\_A] was appended to the first object, :[OBJECT\_B] to the second object. Also, each object was highlighted in a different color. Example sentences are shown in table \ref{tbl:pre2s}. In this way, the annotators could quickly see the objects of interest while the sense of the sentence remains intact.
% Beispielsätze PreStudy 2. Teil
\begin{table}[h]
\centering
\caption{Sentences for the second step}
\label{tbl:pre2s}
\begin{tabular}{{p{12cm}p{3cm}}}
\toprule
Sentence                                                                                                           & Expected Class \\ \midrule
I'd go with \textbf{{\color[HTML]{9A14B2} python:{[}OBJECT\_A{]}}} or \textbf{{\color[HTML]{6CB219}ruby:{[}OBJECT\_B{]}}}.                                 & NO\_COMP       \\
I prefer \textbf{{\color[HTML]{9A14B2}ruby:{[}OBJECT\_A{]}}} over \textbf{{\color[HTML]{6CB219}python:{[}OBJECT\_B{]}}} on windows.                                              & BETTER         \\
I've tried \textbf{{\color[HTML]{9A14B2}python:{[}OBJECT\_A{]}}}, and can see why people like it, but \textbf{{\color[HTML]{6CB219}ruby:{[}OBJECT\_B{]}}} suits my style better. & WORSE          \\
i think this car is a far better deal than the \textbf{{\color[HTML]{9A14B2}bmw{:[OBJECT\_A]}}} 5 series or \textbf{{\color[HTML]{6CB219}mercedes:[OBJECT\_B]}} 320e.                                                                                                                &        UNCLEAR        \\ \bottomrule
\end{tabular}
\end{table}

\label{sec:annotation-guidelines}
\subsection{Results}
Each sentence was annotated by three annotators. Figure \ref{pre:dist} shows the class distribution.

\begin{figure}[h]
\centering
\caption{Class Distribution in the prestudy}
\label{pre:dist}
\begin{tikzpicture}
\pie [rotate=180, text = legend, color= {cgray, cgreen, cred, cblue}]
    {59.76/NO\_COMP (150),
    23.11/BETTER (58),
    9.16/WORSE (23),
    7.97/UNCLEAR (20)}
\end{tikzpicture}
\end{figure}


%\includegraphics[scale=0.6]{images/prestudy/label_distribution.pdf}


Crowdflower has a trust value for each annotator. This trust value and the number of votes per class gives a value of confidence for each label.\footnote{How the confidence is calculated in detail can be found at https://success.crowdflower.com/hc/en-us/articles/201855939-How-to-Calculate-a-Confidence-Score (Last checked: 19.12.2017)}


As presented in figure \ref{pre:conf}, a majority (151) of the labelings has a confidence greater or equal to 0.9, and 15 sentences a confidence below 0.6; the mean is 0.86. Detailed numbers on the confidence are shown in table \ref{pre:conf-table}

\begin{figure}
\centering
\caption{Confidence histogram}
\label{pre:conf}
\includegraphics[scale=0.6]{images/prestudy/confidence.pdf}
\end{figure}


\begin{figure}[h]
\centering
\caption{Confidence}
\begin{tabular}{@{}ll@{}}
\toprule
Type & Value  \\ \midrule
Average Confidence & 0.86 \\
Standard Derivation & 0.17 \\
Lowest Confidence & 0.35\\
Highest Confidence & 1.00\\
25th percentile average & 0.67\\
50th percentile average & 1.00\\
\bottomrule
\label{pre:conf-table}
\end{tabular}
\end{figure}





The most difficult sentence is with a confidence of 0.35 for the class \emph{WORSE} was
\begin{quote}
Google shouldn't have mandated an inferior map app on the iphone:[OBJECT\_A] (as opposed to android:[OBJECT\_B]).
\end{quote}

It was labelled as \emph{BETTER} (trust: 0.72), \emph{WORSE} (trust: 0.85) and \emph{NO\_COMP} (trust: 0.82). The class \emph{WRONG} is correct here, as the object \enquote{iphone} is inferior to \enquote{android} on the aspect of \enquote{map app}.

The following sentence was assigned to \emph{BETTER} (0.37 confidence), although it should belong to \emph{UNCLEAR}.
\begin{quote}
Not to mention that the iphone:[OBJECT\_A] and android:[OBJECT\_B] phones deliver a far superior user experience overall
\end{quote}
However, the annotator for \emph{UNCLEAR} only had 0.87 trust, while the one for \emph{BETTER} had 1 (third one was \emph{NO\_COMP} with 0.82 trust).\newline

All things considered, the result of the prestudy is satisfactory. The annotators agreed in the majority of decisions. 


\newpage
\section{Main Study}
\subsection{Task Description}
\subsection{Data Generation}
Three domains were fixed for the sentences of the main study. The domains were chosen in a way that a majority of people can decide whether a sentence contains a comparison or not.

The most specific domain was "Computer Science Concepts". It contains objects like programming languages, database products and technology standards such as Bluetooth and Ethernet.  Many computer science concepts can be compared objectively, for instance, one can compare Bluetooth and Ethernet on their transmission speed. Some basic knowledge of computer science was needed to label sentences correctly. For example, to compare Eclipse and NetBeans, the annotator must know what an Integrated Development Environment (IDE) is and that both objects are Java IDEs.  The need of the knowledge was communicated to the prospective annotators. The objects for this domain were manually extracted from "List of ..." articles from Wikipedia.

The second, broader domain was "Brands". It contains objects from of different types (car brands, electronics brands, and food). As brands are present in everyday life of people, it is expected that anyone can label the majority of sentences containing well known brands such as Coca-Cola or Mercedes. As with computer science, the objects for this domain were extracted from "List of ..." articles from Wikipedia.

The last domain is not restricted to any topic. For each one of 25 randomly selected seed words, ten similar words were extracted using JoBimText, a software package for distributional semantics. The seed words were created using https://randomlists.com\footnote{Last checked: 25.01.2018}. Listing \ref{lst:jbtres} shows the result\footnote{http://ltmaggie.informatik.uni-hamburg.de/jobimviz/ws/api/stanford/jo/similar/harvard\%23NP?numberOfEntries=10&format=json Last checked: 25.01.2018; Some uninteresting fields were removed for brevity} for the seed word \emph{harvard}.

\begin{lstlisting}[language=json,label=lst:jbtres,caption=Similar words to "Harvard"]
{
   "results":[
      { "score":688.0, "key":"harvard#NP" },
      { "score":245.0, "key":"yale#NP" },
      { "score":163.0, "key":"princeton#NP" },
      { "score":152.0, "key":"mit#NP" },
      { "score":143.0, "key":"stanford#NP" },
      { "score":133.0, "key":"university#NP"},
      { "score":132.0, "key":"tufts#NP" },
      { "score":130.0,"key":"cornell#NP"},
      { "score":127.0, "key":"nyu#NP" },
      { "score":113.0, "key":"university#NN" }
   ]
}
\end{lstlisting}
This method covers a wide are of possible comparison patterns.\newline

Especially for brands and computer science, the object lists are long (4493 brands and 1339 for computer science).The frequency of each object was checked using a frequency dictionary to reduce the number of possible pairs. All objects with a frequency of zero and ambiguous objects were removed from the list. For instance, the objects "RAID" (a hardware concept) and "Unity" (a game engine) were removed from the computer science list as they are also regularly used nouns.

The remaining objects were combined to pairs. For each type, all possible combinations were created. For brands and computer science, the type is the source list. For the unrestricted domain, the seed word was used. This procedure guarantees that only meaningful pairs are created.
The ElasticSearch Index was then queried for entries containing both objects of each pair. For 90\% of the queries, the marker terms where added to the query. This was done to check whether there is a chance that those two objects were compared. All pairs were the query yielded at least 100 sentences were kept. Those pairs are frequent enough and have a high chance of generating comparative sentences.

From the sentences of those pairs, 2500 for each category were randomly sampled as candidates for the crowdsourcing campaign. 250 sentences were manually labelled to check if there are enough comparative sentences. Those labels were discarded for the crowdsourcing campaign.
The label distribution of the 250 sentences is presented in the figures FIGURE NUMBERS.

\begin{figure}[h]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
       \begin{tikzpicture}
\pie [rotate=180,radius=2,color= {cgray, cgreen, cred, cblue}]
    {68.9/NONE,
    15.2/BETTER,
    5.7/WORSE,
    10.2/OTHER }
    \label{pre:brands}
\end{tikzpicture}        \caption{Brands}
    \end{minipage}\hfill
    \begin{minipage}{0.49\textwidth}
        \centering
      \begin{tikzpicture}
\pie [rotate=180,radius=2, color= {cgray, cgreen, cred, cblue}]
    {53.00/NONE,
    19.70/BETTER,
    11.60/WORSE,
    15.70/OTHER }
\end{tikzpicture}
        \caption{Computer Science}
    \end{minipage}
    \end{figure}
    
    \begin{figure}[h]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
       \begin{tikzpicture}
\pie [rotate=180,radius=2,color= {cgray, cgreen, cred, cblue}]
    {65.50/NONE,
    16.10/BETTER,
    7.20/WORSE,
    11.20/OTHER }
\end{tikzpicture}        \caption{Unrestricted}
    \end{minipage}\hfill
    \end{figure}
    
In all samples, at least 30\% of the sentences are comparative. This number shows that the sampling method is sufficient to sample sentences for the crowdsourcing campaign.
