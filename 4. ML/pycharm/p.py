# Top 100 Unigrams  Replace TF IDF
[(0.86454237870893469,
  'middle part replace Range (1, 1) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.86454237870893469,
  'middle part replace Range (1, 1) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.8631557630840434,
  'middle part replace Range (1, 1) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.8631557630840434,
  'middle part replace Range (1, 1) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),

 # Uni-, Bi, and Trigrams, 2500-alle
 (0.8622254433778781,
  'middle part replace Range (1, 3) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.8622254433778781,
  'middle part replace Range (1, 3) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 # Top 100 Unigrams Binary Remove
 (0.86208860812356269,
  'middle part remove Range (1, 1) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.86208860812356269,
  'middle part remove Range (1, 1) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 # 1-3
 (0.86202594010657918,
  'middle part replace Range (1, 3) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.86202594010657918,
  'middle part replace Range (1, 3) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85902518441240983,
  'middle part remove Range (1, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85902518441240983,
  'middle part remove Range (1, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85856500150162574,
  'middle part replace Range (1, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85856500150162574,
  'middle part replace Range (1, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85845628923960404,
  'middle part remove Range (1, 3) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85814417777899643,
  'middle part remove Range (1, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85814417777899643,
  'middle part remove Range (1, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85779820893500436,
  'middle part replace Range (1, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85779820893500436,
  'middle part replace Range (1, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85765667096363962,
  'middle part remove Range (1, 1) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85761308823726179,
  'middle part replace Range (1, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85761308823726179,
  'middle part replace Range (1, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85684582808277021,
  'middle part remove Range (1, 1) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85684582808277021,
  'middle part remove Range (1, 1) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85678835815859233,
  'middle part remove Range (1, 3) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85678835815859233,
  'middle part remove Range (1, 3) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85665945916416686,
  'middle part replace Range (1, 1) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85665945916416686,
  'middle part replace Range (1, 1) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85641526514299793,
  'middle part Range (1, 1) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85641526514299793,
  'middle part remove dist Range (1, 1) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85641526514299793,
  'middle part Range (1, 1) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85641526514299793,
  'middle part remove dist Range (1, 1) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85626695045959211,
  'middle part replace Range (1, 1) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85626695045959211,
  'middle part remove Range (1, 1) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85626695045959211,
  'middle part replace Range (1, 1) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85626695045959211,
  'middle part remove Range (1, 1) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85605584212163177,
  'middle part remove Range (1, 1) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85605584212163177,
  'middle part remove Range (1, 1) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85596306245601794,
  'middle part remove Range (1, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85596306245601794,
  'middle part remove Range (1, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85591304779018951,
  'middle part Range (1, 1) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85591304779018951,
  'middle part remove dist Range (1, 1) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85591304779018951,
  'middle part Range (1, 1) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85591304779018951,
  'middle part remove dist Range (1, 1) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85585643216770801,
  'middle part replace Range (1, 1) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.855655490053565,
  'middle part remove Range (1, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.855655490053565,
  'middle part remove Range (1, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.8554171194663639,
  'middle part remove Range (1, 1) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85532512432629204,
  'middle part replace Range (1, 1) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85507421616598733,
  'middle part Range (1, 3) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85507421616598733,
  'middle part remove dist Range (1, 3) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85507421616598733,
  'middle part Range (1, 3) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85507421616598733,
  'middle part remove dist Range (1, 3) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85476557886818938,
  'middle part replace Range (1, 1) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85476557886818938,
  'middle part remove Range (1, 1) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85476557886818938,
  'middle part replace Range (1, 1) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85476557886818938,
  'middle part remove Range (1, 1) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85475371098908837,
  'middle part Range (1, 1) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85475371098908837,
  'middle part remove dist Range (1, 1) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85475371098908837,
  'middle part Range (1, 1) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85475371098908837,
  'middle part remove dist Range (1, 1) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85466668386457123,
  'middle part Range (1, 1) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85466668386457123,
  'middle part remove dist Range (1, 1) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85466668386457123,
  'middle part Range (1, 1) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85466668386457123,
  'middle part remove dist Range (1, 1) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85443815559119674,
  'middle part Range (1, 3) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85443815559119674,
  'middle part remove dist Range (1, 3) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85348221522357448,
  'middle part remove Range (1, 3) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85339819792444471,
  'middle part Range (1, 1) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85339819792444471,
  'middle part remove dist Range (1, 1) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85339819792444471,
  'middle part Range (1, 1) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85339819792444471,
  'middle part remove dist Range (1, 1) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85289172653764456,
  'middle part replace Range (1, 1) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85289172653764456,
  'middle part replace Range (1, 1) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85287378932551028,
  'middle part Range (1, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85287378932551028,
  'middle part remove dist Range (1, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85287378932551028,
  'middle part Range (1, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85287378932551028,
  'middle part remove dist Range (1, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85262956752186114,
  'middle part Range (1, 3) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85262956752186114,
  'middle part remove dist Range (1, 3) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.8526166207624476,
  'middle part Range (1, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.8526166207624476,
  'middle part remove dist Range (1, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.8526166207624476,
  'middle part Range (1, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.8526166207624476,
  'middle part remove dist Range (1, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.8522698839240449,
  'middle part Range (1, 3) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.8522698839240449,
  'middle part remove dist Range (1, 3) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85211349236492084,
  'middle part remove Range (1, 1) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85211349236492084,
  'middle part remove Range (1, 1) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85207829916043298,
  'middle part Range (1, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85207829916043298,
  'middle part remove dist Range (1, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85207829916043298,
  'middle part Range (1, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85207829916043298,
  'middle part remove dist Range (1, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85173325151516688,
  'middle part Range (1, 1) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85173325151516688,
  'middle part remove dist Range (1, 1) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85173325151516688,
  'middle part Range (1, 1) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85173325151516688,
  'middle part remove dist Range (1, 1) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.8514051069970946,
  'middle part remove Range (1, 3) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.8514051069970946,
  'middle part remove Range (1, 3) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85133536569478097,
  'middle part replace Range (1, 3) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85123974663684243,
  'middle part remove Range (1, 1) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85123974663684243,
  'middle part remove Range (1, 1) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85037569353954401,
  'middle part replace Range (1, 3) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.85033054811494646,
  'middle part replace Range (1, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85033054811494646,
  'middle part replace Range (1, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85028652506863667,
  'middle part Range (1, 1) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85028652506863667,
  'middle part remove dist Range (1, 1) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85028652506863667,
  'middle part Range (1, 1) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85028652506863667,
  'middle part remove dist Range (1, 1) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.85005034825240333,
  'middle part replace Range (1, 1) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.85005034825240333,
  'middle part replace Range (1, 1) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.84974138690934897,
  'middle part remove Range (1, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.84974138690934897,
  'middle part remove Range (1, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.84946066837336609,
  'middle part remove Range (1, 1) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.84946066837336609,
  'middle part remove Range (1, 1) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.84941365188869022,
  'middle part remove Range (1, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.84941365188869022,
  'middle part remove Range (1, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.84923766833179326,
  'middle part replace Range (1, 1) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.84923766833179326,
  'middle part replace Range (1, 1) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.84850368881685767,
  'middle part Range (1, 1) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.84850368881685767,
  'middle part remove dist Range (1, 1) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.84850368881685767,
  'middle part Range (1, 1) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.84850368881685767,
  'middle part remove dist Range (1, 1) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.84837498312537474,
  'middle part Range (1, 1) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.84837498312537474,
  'middle part remove dist Range (1, 1) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.84798131185624692,
  'middle part Range (1, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.84798131185624692,
  'middle part remove dist Range (1, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.84798131185624692,
  'middle part Range (1, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.84798131185624692,
  'middle part remove dist Range (1, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.84773040262313515,
  'middle part Range (1, 3) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.84773040262313515,
  'middle part remove dist Range (1, 3) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.84726892560608713,
  'middle part Range (1, 1) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.84726892560608713,
  'middle part remove dist Range (1, 1) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.84705364631771829,
  'middle part replace Range (1, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.84705364631771829,
  'middle part replace Range (1, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.84658891014413884,
  'middle part replace Range (1, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.84658891014413884,
  'middle part replace Range (1, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.845452133617421,
  'middle part Range (1, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.845452133617421,
  'middle part remove dist Range (1, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.845452133617421,
  'middle part Range (1, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.845452133617421,
  'middle part remove dist Range (1, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.84275604173833707,
  'middle part replace Range (2, 2) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.84275604173833707,
  'middle part replace Range (2, 2) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.84246415333947988,
  'middle part replace Range (2, 2) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.84246415333947988,
  'middle part replace Range (2, 2) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.83900015886355972,
  'middle part Range (1, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.83900015886355972,
  'middle part remove dist Range (1, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.83900015886355972,
  'middle part Range (1, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.83900015886355972,
  'middle part remove dist Range (1, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),

 # Bigrams
 (0.83885319439252004,
  'middle part replace Range (2, 2) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.83885319439252004,
  'middle part replace Range (2, 2) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.83687326288121155,
  'middle part replace Range (2, 2) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.83687326288121155,
  'middle part replace Range (2, 2) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.83665168833763537,
  'middle part replace Range (2, 2) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.83568756610289796,
  'middle part replace Range (2, 2) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.83372135620859567,
  'middle part replace Range (2, 2) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.83372135620859567,
  'middle part replace Range (2, 2) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.83272915388406721,
  'middle part replace Range (2, 2) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.83272915388406721,
  'middle part replace Range (2, 2) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.83220991225280783,
  'middle part replace Range (2, 2) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.83220991225280783,
  'middle part replace Range (2, 2) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.82857854996956826,
  'middle part replace Range (2, 2) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.82857854996956826,
  'middle part replace Range (2, 2) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 # full sentence
 (0.82353604326586671,
  'full sentence replace Range (1, 3) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.82138879786547447,
  'full sentence replace Range (1, 3) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.81934682650614055,
  'full sentence replace Range (1, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.81934682650614055,
  'full sentence replace Range (1, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.81917990088461101,
  'full sentence replace Range (1, 3) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.81904849138184777,
  'full sentence replace Range (1, 3) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.8188543838882324,
  'full sentence replace Range (1, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.8188543838882324,
  'full sentence replace Range (1, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.81846301445159464,
  'full sentence replace Range (1, 3) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.81466146128207451,
  'full sentence replace Range (1, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.81466146128207451,
  'full sentence replace Range (1, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.81442683578868935,
  'full sentence replace Range (1, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.81442683578868935,
  'full sentence replace Range (1, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.81343013772361616,
  'middle part remove Range (2, 2) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.81341711926517191,
  'full sentence replace Range (1, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.81341711926517191,
  'full sentence replace Range (1, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.81243067037154093,
  'full sentence replace Range (1, 3) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.81202089362711349,
  'middle part Range (2, 2) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.81202089362711349,
  'middle part remove dist Range (2, 2) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.81202089362711349,
  'middle part Range (2, 2) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.81202089362711349,
  'middle part remove dist Range (2, 2) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.81188610916432791,
  'middle part remove Range (2, 2) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.81188610916432791,
  'middle part remove Range (2, 2) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.81188610916432791,
  'middle part remove Range (2, 2) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.81096114040510237,
  'middle part remove Range (2, 2) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.81096114040510237,
  'middle part remove Range (2, 2) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.81061039865167606,
  'middle part Range (2, 2) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.81061039865167606,
  'middle part remove dist Range (2, 2) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.81061039865167606,
  'middle part Range (2, 2) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.81061039865167606,
  'middle part remove dist Range (2, 2) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.80977548518343445,
  'full sentence replace Range (1, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.80977548518343445,
  'full sentence replace Range (1, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.80947757323810254,
  'middle part remove Range (2, 2) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.80947757323810254,
  'middle part remove Range (2, 2) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.80926483090516299,
  'middle part Range (2, 2) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.80926483090516299,
  'middle part remove dist Range (2, 2) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.80926483090516299,
  'middle part Range (2, 2) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.80926483090516299,
  'middle part remove dist Range (2, 2) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.80853916099587797,
  'middle part remove Range (2, 2) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.80853916099587797,
  'middle part remove Range (2, 2) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.80816956278972707,
  'middle part Range (2, 2) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.80816956278972707,
  'middle part remove dist Range (2, 2) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.80816956278972707,
  'middle part Range (2, 2) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.80816956278972707,
  'middle part remove dist Range (2, 2) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.80748760588699731,
  'middle part remove Range (2, 2) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.80748760588699731,
  'middle part remove Range (2, 2) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.80746129233417452,
  'middle part remove Range (2, 2) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.80746129233417452,
  'middle part remove Range (2, 2) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.80717983839906249,
  'middle part Range (2, 2) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.80717983839906249,
  'middle part remove dist Range (2, 2) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.80717983839906249,
  'middle part Range (2, 2) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.80717983839906249,
  'middle part remove dist Range (2, 2) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.80639748995139693,
  'middle part Range (2, 2) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.80639748995139693,
  'middle part remove dist Range (2, 2) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.80639748995139693,
  'middle part Range (2, 2) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.80639748995139693,
  'middle part remove dist Range (2, 2) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.80594692908168941,
  'middle part remove Range (2, 2) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.80594692908168941,
  'middle part remove Range (2, 2) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.80420553128466588,
  'middle part Range (2, 2) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.80420553128466588,
  'middle part remove dist Range (2, 2) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.80420553128466588,
  'middle part Range (2, 2) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.80420553128466588,
  'middle part remove dist Range (2, 2) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.80374226440488239,
  'middle part remove Range (2, 2) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.80374226440488239,
  'middle part remove Range (2, 2) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.80335133589102325,
  'full sentence replace Range (2, 2) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.80335133589102325,
  'full sentence replace Range (2, 2) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.8025254896620998,
  'full sentence replace Range (2, 2) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.8025254896620998,
  'full sentence replace Range (2, 2) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.80214679628708774,
  'full sentence replace Range (2, 2) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.80214679628708774,
  'full sentence replace Range (2, 2) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.79971529178405687,
  'full sentence replace Range (2, 2) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.79971529178405687,
  'full sentence replace Range (2, 2) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.79944596341975993,
  'full sentence replace Range (2, 2) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.79944596341975993,
  'full sentence replace Range (2, 2) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.79866865893772776,
  'full sentence replace Range (2, 2) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.79866865893772776,
  'full sentence replace Range (2, 2) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.79794189385108927,
  'full sentence replace Range (2, 2) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.79794189385108927,
  'full sentence replace Range (2, 2) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.79764489544066997,
  'middle part Range (2, 2) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.79764489544066997,
  'middle part remove dist Range (2, 2) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.79764489544066997,
  'middle part Range (2, 2) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.79764489544066997,
  'middle part remove dist Range (2, 2) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.79700073851823638,
  'full sentence replace Range (2, 2) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.79592439884925936,
  'full sentence replace Range (2, 2) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.79592439884925936,
  'full sentence replace Range (2, 2) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.79543830645466129,
  'middle part Range (2, 2) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.79543830645466129,
  'middle part remove dist Range (2, 2) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.79543830645466129,
  'middle part Range (2, 2) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.79543830645466129,
  'middle part remove dist Range (2, 2) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.7942146190612972,
  'full sentence replace Range (2, 2) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.79392353385943626,
  'full sentence Range (1, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.79392353385943626,
  'full sentence remove dist Range (1, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.79392353385943626,
  'full sentence Range (1, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.79392353385943626,
  'full sentence remove dist Range (1, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78538437847338116,
  'full sentence Range (1, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.78538437847338116,
  'full sentence remove dist Range (1, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.78538437847338116,
  'full sentence Range (1, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78538437847338116,
  'full sentence remove dist Range (1, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78523620785965564,
  'full sentence Range (1, 1) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.78523620785965564,
  'full sentence remove dist Range (1, 1) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.78523620785965564,
  'full sentence Range (1, 1) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78523620785965564,
  'full sentence remove dist Range (1, 1) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78400695096359374,
  'full sentence replace Range (1, 1) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.78400695096359374,
  'full sentence replace Range (1, 1) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78369337422228114,
  'full sentence replace Range (1, 1) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.78369337422228114,
  'full sentence replace Range (1, 1) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78361363006606854,
  'full sentence remove Range (1, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.78361363006606854,
  'full sentence remove Range (1, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78274738406410871,
  'full sentence remove Range (1, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.78274738406410871,
  'full sentence remove Range (1, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78270056176317282,
  'full sentence remove Range (1, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.78270056176317282,
  'full sentence remove Range (1, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78269787798116852,
  'full sentence replace Range (1, 1) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.78269787798116852,
  'full sentence replace Range (1, 1) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78263710327664104,
  'full sentence Range (1, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.78263710327664104,
  'full sentence remove dist Range (1, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.78263710327664104,
  'full sentence Range (1, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78263710327664104,
  'full sentence remove dist Range (1, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78152259305410887,
  'full sentence Range (1, 3) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.78152259305410887,
  'full sentence remove dist Range (1, 3) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.78148872921682555,
  'full sentence remove Range (1, 1) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.78148872921682555,
  'full sentence remove Range (1, 1) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78130703549897984,
  'full sentence Range (1, 1) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.78130703549897984,
  'full sentence remove dist Range (1, 1) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.78130703549897984,
  'full sentence Range (1, 1) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78130703549897984,
  'full sentence remove dist Range (1, 1) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78089176797436155,
  'full sentence remove Range (1, 1) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.78072227483762457,
  'full sentence remove Range (1, 1) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.78072227483762457,
  'full sentence remove Range (1, 1) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78044448106878594,
  'full sentence remove Range (1, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.78044448106878594,
  'full sentence remove Range (1, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.78041243196098287,
  'full sentence replace Range (1, 1) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.78020362621906125,
  'full sentence Range (1, 1) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.78020362621906125,
  'full sentence remove dist Range (1, 1) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.78012837976751603,
  'full sentence Range (1, 1) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.78012837976751603,
  'full sentence remove dist Range (1, 1) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77999882910821394,
  'full sentence remove Range (1, 3) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77951941553371074,
  'full sentence Range (1, 1) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77951941553371074,
  'full sentence remove dist Range (1, 1) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77951941553371074,
  'full sentence Range (1, 1) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77951941553371074,
  'full sentence remove dist Range (1, 1) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77911683838169754,
  'full sentence remove Range (1, 1) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77911683838169754,
  'full sentence remove Range (1, 1) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77891212799821208,
  'full sentence Range (1, 3) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77891212799821208,
  'full sentence remove dist Range (1, 3) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.7788537112350391,
  'full sentence Range (1, 1) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.7788537112350391,
  'full sentence remove dist Range (1, 1) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.7788537112350391,
  'full sentence Range (1, 1) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.7788537112350391,
  'full sentence remove dist Range (1, 1) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77884783582079864,
  'full sentence Range (1, 3) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77884783582079864,
  'full sentence remove dist Range (1, 3) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77860204973328584,
  'full sentence Range (1, 3) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77860204973328584,
  'full sentence remove dist Range (1, 3) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77851374126512229,
  'full sentence Range (1, 1) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77851374126512229,
  'full sentence remove dist Range (1, 1) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.7783281290771823,
  'full sentence Range (1, 3) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.7783281290771823,
  'full sentence remove dist Range (1, 3) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77831175449752976,
  'full sentence Range (1, 1) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77831175449752976,
  'full sentence remove dist Range (1, 1) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77790030651101794,
  'full sentence remove Range (1, 3) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77730048290511045,
  'full sentence remove Range (1, 1) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77705114567838984,
  'full sentence replace Range (1, 1) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77691311542906394,
  'full sentence remove Range (1, 3) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77659028939668417,
  'full sentence remove Range (1, 1) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77659028939668417,
  'full sentence remove Range (1, 1) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77655578887653887,
  'full sentence Range (1, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77655578887653887,
  'full sentence remove dist Range (1, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77655578887653887,
  'full sentence Range (1, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77655578887653887,
  'full sentence remove dist Range (1, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77649025398673988,
  'full sentence remove Range (1, 3) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77617420611903587,
  'full sentence Range (1, 1) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77617420611903587,
  'full sentence remove dist Range (1, 1) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77617420611903587,
  'full sentence Range (1, 1) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77617420611903587,
  'full sentence remove dist Range (1, 1) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77616331457684506,
  'full sentence replace Range (1, 1) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77616331457684506,
  'full sentence replace Range (1, 1) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.7755454630011922,
  'full sentence remove Range (1, 1) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.7755454630011922,
  'full sentence remove Range (1, 1) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77550896612198927,
  'full sentence Range (1, 1) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77550896612198927,
  'full sentence remove dist Range (1, 1) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77550896612198927,
  'full sentence Range (1, 1) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77550896612198927,
  'full sentence remove dist Range (1, 1) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77544399078765791,
  'full sentence Range (1, 3) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77544399078765791,
  'full sentence remove dist Range (1, 3) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77488832249180062,
  'full sentence replace Range (1, 1) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77488832249180062,
  'full sentence replace Range (1, 1) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77467480396685107,
  'full sentence remove Range (1, 3) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77428463605990494,
  'full sentence remove Range (1, 3) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77419476964724709,
  'full sentence remove Range (1, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77419476964724709,
  'full sentence remove Range (1, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77363227913501831,
  'full sentence Range (1, 1) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77363227913501831,
  'full sentence remove dist Range (1, 1) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77334774439764986,
  'full sentence Range (1, 1) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77334774439764986,
  'full sentence remove dist Range (1, 1) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77286492405384866,
  'full sentence remove Range (1, 1) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77286492405384866,
  'full sentence remove Range (1, 1) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77266924653826352,
  'full sentence replace Range (1, 1) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77266924653826352,
  'full sentence replace Range (1, 1) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77264843487442181,
  'full sentence Range (1, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77264843487442181,
  'full sentence remove dist Range (1, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77264843487442181,
  'full sentence Range (1, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77264843487442181,
  'full sentence remove dist Range (1, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.7720483984905836,
  'full sentence replace Range (1, 1) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.7720483984905836,
  'full sentence remove Range (1, 1) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77130922532907853,
  'full sentence Range (1, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77130922532907853,
  'full sentence remove dist Range (1, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77130922532907853,
  'full sentence Range (1, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77130922532907853,
  'full sentence remove dist Range (1, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.77035045456606888,
  'full sentence replace Range (1, 1) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77035045456606888,
  'full sentence remove Range (1, 1) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77035045456606888,
  'full sentence replace Range (1, 1) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77035045456606888,
  'full sentence remove Range (1, 1) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.77005384946824351,
  'full sentence remove Range (1, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.77005384946824351,
  'full sentence remove Range (1, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.76956223892987019,
  'full sentence replace Range (1, 1) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.76956223892987019,
  'full sentence remove Range (1, 1) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.76165525096190823,
  'middle part replace Range (3, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.76165525096190823,
  'middle part replace Range (3, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.76163639111294845,
  'middle part replace Range (3, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.76163639111294845,
  'middle part replace Range (3, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.76043533746133585,
  'middle part replace Range (3, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.76043533746133585,
  'middle part replace Range (3, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.75875383957262543,
  'middle part replace Range (3, 3) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.75823991997241902,
  'middle part replace Range (3, 3) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.75823991997241902,
  'middle part replace Range (3, 3) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.75823991997241902,
  'middle part replace Range (3, 3) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.75823991997241902,
  'middle part replace Range (3, 3) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.75709069938939155,
  'middle part replace Range (3, 3) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.75671902069081798,
  'middle part replace Range (3, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.75671902069081798,
  'middle part replace Range (3, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.75066380864049942,
  'full sentence replace Range (3, 3) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.74833767107583249,
  'full sentence replace Range (3, 3) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.74683841293868503,
  'middle part replace Range (3, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.74683841293868503,
  'middle part replace Range (3, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.74683841293868503,
  'middle part replace Range (3, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.74683841293868503,
  'middle part replace Range (3, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.74652334695819855,
  'full sentence replace Range (3, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.74652334695819855,
  'full sentence replace Range (3, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.74652334695819855,
  'full sentence replace Range (3, 3) Binary True Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.74652334695819855,
  'full sentence replace Range (3, 3) Binary False Top 100 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.7437047670958955,
  'full sentence replace Range (3, 3) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.7437047670958955,
  'full sentence replace Range (3, 3) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.7437047670958955,
  'full sentence replace Range (3, 3) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.7437047670958955,
  'full sentence replace Range (3, 3) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.74240027700767686,
  'full sentence replace Range (3, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.74240027700767686,
  'full sentence replace Range (3, 3) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.74224538336087087,
  'full sentence replace Range (3, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.74224538336087087,
  'full sentence replace Range (3, 3) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.74050661109819671,
  'full sentence replace Range (3, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.74050661109819671,
  'full sentence replace Range (3, 3) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.7308821965254988,
  'full sentence replace Range (3, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.7308821965254988,
  'full sentence replace Range (3, 3) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.72653778019123938,
  'full sentence remove Range (2, 2) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.72653778019123938,
  'full sentence remove Range (2, 2) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.72573993105276213,
  'full sentence Range (2, 2) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.72573993105276213,
  'full sentence remove dist Range (2, 2) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.72573993105276213,
  'full sentence Range (2, 2) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.72573993105276213,
  'full sentence remove dist Range (2, 2) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.72466425644359567,
  'full sentence remove Range (2, 2) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.72466425644359567,
  'full sentence remove Range (2, 2) Binary False Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.72359153581671976,
  'full sentence remove Range (2, 2) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.72359153581671976,
  'full sentence remove Range (2, 2) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.72039429527157683,
  'full sentence remove Range (2, 2) Binary True Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.71988248975709146,
  'full sentence Range (2, 2) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.71988248975709146,
  'full sentence remove dist Range (2, 2) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.71972062899941125,
  'full sentence remove Range (2, 2) Binary True Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.71907300841482014,
  'full sentence Range (2, 2) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.71907300841482014,
  'full sentence remove dist Range (2, 2) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.71907300841482014,
  'full sentence Range (2, 2) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.71907300841482014,
  'full sentence remove dist Range (2, 2) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.71893048809782845,
  'full sentence remove Range (2, 2) Binary False Top 2500 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.71779888136398462,
  'full sentence Range (2, 2) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.71779888136398462,
  'full sentence remove dist Range (2, 2) Binary False Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.71765930003288014,
  'full sentence remove Range (2, 2) Binary False Top None '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.71724263380658471,
  'full sentence Range (2, 2) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.71724263380658471,
  'full sentence remove dist Range (2, 2) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.71724263380658471,
  'full sentence Range (2, 2) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.71724263380658471,
  'full sentence remove dist Range (2, 2) Binary True Top None '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.71455986989057707,
  'full sentence remove Range (2, 2) Binary True Top 100 '
  "(CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  '        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n'
  "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n"
  '        tokenizer=None, vocabulary=None) )'),
 (0.71361113929712539,
  'full sentence remove Range (2, 2) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.71361113929712539,
  'full sentence remove Range (2, 2) Binary False Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) smoothed)'),
 (0.7114608991722764,
  'full sentence Range (2, 2) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
  "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, "
  'use_idf=True,\n'
  '        vocabulary=None) )'),
 (0.7114608991722764,
  'full sentence remove dist Range (2, 2) Binary True Top 2500 '
  "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n"
  "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n"
  '        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n'
  "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n"
  '        stop_words=None, strip_accents=None, sublinear_tf=False,\n'
